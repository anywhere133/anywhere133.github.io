---
title: "YOLO v1 논문 리뷰"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
  - 논문
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
  - YOLO
  - YOLOv1
use_math: true
---

이 글은 [YOLOv1 논문 (You Only Look Once : Unified, Real-Time Object Detection)](https://arxiv.org/pdf/1506.02640.pdf)을 처음부터 리뷰합니다.  

## 도입

Object Detection에 대한 새로운 접근법 (One-Stage Object Detection)이 처음 제안된 논문입니다.  
2015년 당시, 대부분의 detection system들은 classifier들이 detection의 기능을 수행하도록 repurpose되어 있었고,  
Sliding window를 사용하는 defromable parts model (DPM), 그리고 당시 가장 최근의 접근인 Region Proposal method를 사용하는 R-CNN 등이 존재하고 있었습니다. (Two-Stage)  
이러한 복잡한 파이프라인을 가진 모델들은 각 개별 요소가 분리되어 학습되어야 했기 때문에 느리고 최적화하기 어려웠습니다.

저자들은 객체 탐지를 단순 회귀 문제로 재구성하여 이미지 픽셀들로부터 직접적으로 bounding box와 class probabilities를 구할 수 있도록 했습니다.  
말 그대로 논문 제목처럼 한 번만 보기만 하면(You Only look once) detection이 이루어지는 것입니다.  

![](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov1_fig1.JPG?raw=true)

YOLO는 참신하게도 단순하다: Figure 1을 보라.  
단일 convolutional network가 동시에 다수의 bounding box와 그 box들에 대한 class probabilities를 predict한다.  
YOLO는 전체 이미지에 대해 학습하고, 바로 detection 성능을 최적화한다.  
이 단일화된 모델은 Object Detection의 전통적인 방법이 가지지 못하는 여러 장점들이 있다.

첫 번째, **YOLO는 굉장히 빠르다.**  
detection을 회귀 문제로 구성했기 때문에, 복잡한 파이프라인이 필요하지 않다.  
단순히 detection을 predict하기 위해 테스트 시간에 새 이미지를 신경망에 돌려보면 된다.  
기본 신경망은 Titan X GPU에서 배치 처리 없이 초당 45프레임의 속도로 작동하며,  
빠른 버전은 150 fps까지 나온다.  
이것은 25ms보다 더 낮은 레이턴시로 real-time 스트리밍 비디오를 처리할 수 있다는 의미이다.  
게다가 YOLO는 다른 real-time system의 2배 이상의 mAP(mean Average Precision)를 달성했다.  

두 번째, **예측할 때, 이미지에 대해 전체적으로 판단한다.**  
slinding window나 region proposal 기반 테크닉과는 달리, YOLO는 학습하는 동안 이미지 전체를 보기 때문에 class 뿐만 아니라 그것의 외양에 대한 맥락적인 정보를 함축적으로 부호화한다.  
Fast R-CNN은 더 큰 맥락에서 볼 수 없기 때문에 이미지에서 객체와 배경의 분리에서 실수가 일어난다.  
YOLO는 Fast R-CNN에 비교했을 때, background error의 수가 절반으로 줄어든다.

세 번째, **YOLO는 객체의 일반화할 수 있는 표상을 학습한다. ** 
자연 이미지를 학습하고, 예술 작품에 테스트할 때, DPM과 R-CNN와 같은 탐지 모델보다 큰 폭으로 더 잘 수행한다.  
YOLO가 굉장히 일반화 가능하기 때문에, 새로운 도메인이나 예상하지 못한 입력을 적용했을 때 성능이 훨씬 덜 떨어지는 경향을 보인다.

YOLO는 최신 detection system들에 비해 accuracy에 있어서 아직 뒤쳐져있다.  
반면에 이미지에서 객체를 빠르게 찾아낼 수 있지만 작은 객체들과 같은 몇몇 객체들을 정확하게 위치를 찾는 것에 어려움이 있다.

## 단일화된 Detection

Object Detection에서 분리된 요소들을 하나의 신경망으로 통합시켰다.  
YOLO는 각 bounding box를 예측하기 위해 이미지의 전체에서 feature들을 사용한다.  
또한 이미지에 대해 동시에 모든 class에 걸쳐 모든 bounding box들을 예측한다.  
이 것은 신경망이 이미지 전체와 이미지 안에 객체 모두에 대해 전체적으로 추론한다는 것을 의미한다.   
YOLO 디자인은 높은 AP(Average Precision)을 유지하는 동시에 end-to-end 학습과 real-time speed를 가능하게 만들었다.

YOLO system은 입력 이미지를 $ S \times S$ 격자로 나누었다.  
만약 객체의 중심이 격자 cell에 맞아 떨어지면, 해당 격자 cell은 객체를 detect할 책임을 지게된다. 

각 격자 cell은 *B* bounding box들과 그 box들에 대한 confidence score를 예측한다.  
이 confidence score는 모델이 box가 객체를 포함하고 있는 것을 얼마나 확신(confident)하는지, 그리고 또한 box가 그 객체를 예측하는 것이 얼마나 정확한지를 반영한다.  
confidence score는 $Pr(Object) \times IOU_{pred}^{truth}$로 정의된다.  
만약 cell에 객체가 존재하지 않으면, confidence score는 0이 될 것이다.  
그 외에는 confidence score를 predict box와 ground truth 사이의 Intersection Over Union (IOU)와 같도록 만들고 싶다.

각 bounding box는 5개의 예측값들로 구성되어 있다. : $x, y, w, h, and confidence$  
$(x, y)$ 좌표는 격자 cell의 경계와 관련된 box의 중앙을 나타낸다.  
넓이(width)와 높이(height)는 전체 이미지와 관련되어 predict된다.  
마침내 confidence prediction은 ground truth와 predicted box 사이의 IOU 값을 나타낸다.

각 격자 cell은 또한 *C* class의 조건부 확률을 예측한다. $Pr(Class_{i}|Object)$  
이 확률들은 객체를 포함하고 있는 격자 cell에 조절된다.  
box의 개수 *B*와는 상관 없이, 격자 cell 당 하나의 세트의 class 확률들만을 예측한다.

테스트 타임에서는 class의 조건부 확률과 개별 box의 confidence prediction을 곱한다.

$$ Pr(Class_{i}|Object) \times Pr(Object) \times IOU_{pred}^{truth} = Pr(Class_{i}) \times IOU_{pred}^{truth} $$

위의 식은 각 box에 대한 특정 class의 confidence score를 준다.  
이러한 score는 박스에 나타난 class의 확률과 객체에 predict box가 얼마나 잘 맞는지 둘다를 표현한다.

![https://arxiv.org/pdf/1506.02640.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov1_fig2.JPG?raw=true)

해당 논문에서 Pascal VOC에 대해 YOLO를 평가할 때, $ S = 7, B = 2$였고,  
Pascal VOC는 20개의 클래스가 있으므로, $C = 20$이다.  
예측에서 나오는 최종 tensor는 $7 \times 7 \times 30$ 크기의 tensor이다.

## Network Design

이 모델을 convolutional neural network로 구현하고, Pascal VOC detection dataset으로 평가하였다.  
초기 convolutional layer들의 network는 이미지로부터 feature를 추출하고, fully connected layer들이 좌표와 확률을 예측한다.

YOLO 신경망 구조는 이미지 분류에 대한 모델, GoogLeNet으로부터 영감을 받았다.  
YOLO network는 24개의 convolutional layer와 그에 이어지는 2개의 fully connected layer를 가지고 있다.  
GoogLeNet에서 사용된 인셉션 모듈 대신에, 단순하게 $1 \times 1$ reduction layer에 이은 $3 \times 3$ convolutional layer를 사용한다.

전체 network는 아래와 같다.

![https://arxiv.org/pdf/1506.02640.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov1_fig3.JPG?raw=true)

## 학습(Training)

convolutional layer들을 1000-class competition dataset인 ImageNet에 pretrain시켰다.  
pretrain을 위해 Fig.3에 보이는 첫 20개의 convolutional layer, 그 뒤에 이어지는 average-pooling layer와 fully connected layer를 사용했다.  
이 network를 거의 1주일동안 학습시키고, ImageNet 2012 validation set에 대해 single crop top-5 정확도가 88%로, caffe's model zoo에 있는 GoogLeNet과 비슷한 결과를 내놓았다.  
학습과 추론을 위해서 Darknet 프레임워크를 사용했다.

그 다음, detection을 수행하기 위해 model을 변환시켰다.  
Ren et al. 에서 convolutional, connected layer를 pretrained network에 더하는 것은 성능을 향상시킬 수 있다는 것을 보였다.  
이러한 예를 따라서, 랜덤하게 초기화된 가중치를 가지는 4개의 convolutional layer와 2개의 fully connected layer를 더했다.  
detection은 자주 잘 정제된 시각적 정보를 필요로 하기 때문에, 신경망의 입력 해상도를 224 x 224에서 448 x 448로 증가시켰다.

최종 layer는 bounding box 좌표와 class 확률 둘다 예측한다.  
bounding box의 width와 height를 이미지의 width와 height에 일반화하여, 그 값이 0에서 1 사이의 값 범위에 떨어지도록 만들었다.  
bounding box의 x와 y 좌표 값이 특정 격자 cell 위치의 offset이 되도록 parameterize했으며, 두 값도 0과 1사이의 값을 가지도록 만들었다.

마지막 layer에는 linear activation 함수를 사용했고,  
다른 모든 layer에는 아래와 같은 leaky rectified linear activation을 사용했다.

<p>$$ \phi(x) = 
\begin{cases}
x, & \text{if $x > 0$} \\
0.1x, & \text{otherwise}
\end{cases}$$</p>

모델의 출력을 sum-squared error (SSE)로 최적화하였다.  
SSE를 사용한 이유는 최적화하기 쉽기 때문이지만,  
average precision을 최대화하기 위한 목표에는 정확하게 맞지는 않는다.  
SSE는 localization error와 classification error에 동일한 가중치를 두는 점 때문에 이상적이지 않을 수 있다.  
또한 이미지마다 많은 격자 cell이 어떠한 객체를 포함하고 있지 않는다.  
이는 객체를 포함하고 있는 cell보다 그렇지 않은 (confidence score가 0인) cell들이 더 자주 gradient에 큰 영향력을 끼치도록 만든다.
이것은 학습 중 일찍 error 발산하도록 만들어, 모델 불안정성으로 이끌 수 있다.

이것을 해결하기 위해서, bounding box의 좌표 prediction의 loss를 증가시기고,  
객체를 포함하고 있지 않은 box에 대한 confidence prediction의 loss를 감소시킨다.  
2개의 파라미터, $\lambda_{coord}$와 $\lambda_{noobj}$를 사용하여 달성할 수 있다.  
$\lambda_{coord} = 5$, $\lambda_{noobj} = 0.5$로 설정하였다.

SSE는 또한 큰 크기의 box와 작은 크기의 box에 대한 error에 동일한 가중치를 둔다.  
error metric은 큰 box에서의 작은 오차를 작은 box에서의 작은 오차보다 더 작게 반영해야한다.  
이것을 부분적으로 강조하기 위해서, width와 height를 직접적으로 예측하는 대신 width와 height의 제곱근 값을 사용하여 예측한다.

YOLO는 격자 cell 당 다수의 bounding box를 예측한다.  
학습 단계에서는 하나의 bounding box predictor가 각 객체를 담당하기를 원했다.   
어떤 prediction이 ground truth와 현재 가장 높은 IOU 값을 가지고 있는지에 기반하여, 하나의 predictor가 객체를 predict하는 것을 담당하도록 할당하였다.  
이것은 bounding box 사이의 전문화?(specialization)으로 이끈다.  
각 predictor는 특정 크기, 종횡비, 객체의 class에 대해서 더 잘 predict할 수 있게 되면서, 전체적인 recall도 향상된다.

학습 중에는 아래와 같은 여러 부분의 loss 함수로 최적화한다.

<p>$$\begin{aligned}
& \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \Big[(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2\Big] \\
& + \lambda_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} \Big[(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2\Big] \\
& + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i)^2 \\
& + \lambda_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^{noobj} (C_i - \hat{C}_i )^2 \\
& + \sum_{i=0}^{S^2} \mathbb{1}_{i}^{obj} 
\sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2
\end{aligned}$$</p>

$ \mathbb{1}_ {i}^{obj}$ 은 객체가 cell $i$에 보임을 나타내고,  
$\mathbb{1}_{ij}^{obj} $은 cell $i$의 $j$번째 bounding box predictor가 해당 prediction에 책임이 있음을 나타내준다.

격자 cell 안에 객체가 존재하는 경우에만 classification error에 loss function은 페널티를 준다는 것을 유의하자. (앞서 이야기했던 class의 조건부 확률의 이유로)  
또한 ground truth box에 대해 predictor가 책임이 있는 경우에만 bounding box error에 패널티를 준다. (예를 들어, 해당 격자 cell에서 가장 높은 IOU 값을 가지는 어떤 predictor에 대해서만)
***(즉, 객체가 존재하는 경우에만 classification error를 계산하고, IOU값이 가장 높은 predictor에 대해서만 bounding box corrdinate error를 계산한다는 의미*)**

Pascal VOC 2007과 2012의 training / validation set를 가지고 135 epoch만큼 network를 학습시켰음.  
학습에는 64의 배치사이즈, 0.9의 momentum, 0.0005의 decay의 하이퍼파라미터를 사용했음.

learning rate 스케쥴은 다음과 같았다.  
첫 epoch부터는 $10^{-3}$에서 $10^{-2}$까지 천천히 상승시켰다.  
높은 learning rate로 시작하는 경우에는 불안정한 gradient 때문에 모델이 자주 발산했다.  
75 epoch까지는 $10^{-2}$으로 계속 학습시켰으며, 그 다음 30 epoch는 $10^{-3}$, 마지막 30 epoch에는 $10^{-4}$로 학습시켰다. 

과적합을 피하기 위해서 드롭아웃과 추가적인 data augmentation을 사용했다.  
$rate = .5$의 드롭아웃 layer 뒤에 첫 연결된 layer는 layer 간의 co-adaptation을 방지한다.
data augmentation은 random scaling과 원본 이미지 사이즈의 20%까지의 이동(translation)을 적용하였다.  
또한 HSV 색공간에서 이미지의 채도와 노출을 1.5배까지 랜덤하게 조절했다.

## Inference

학습과 동일하게 테스트 이미지에 대한 detection을 predict하는 것은 하나의 network 평가만 필요하다.  
Pascal VOC에 대해 network는 이미지당 98개의 bounding box와 각 box에 대한 class probability를 predict한다.  
YOLO는 classifier기반 방법과는 다르게, 하나의 network 평가만이 필요하기 떄문에 테스트 시간에 있어서 굉장히 빠르다. 

격자 디자인은 bounding box prediction에 있어서 공간적 다양성을 강화해준다.  
객체가 어떤 격자 cell에 들어맞는 것이 종종 분명하고, network는 각 객체마다 하나의 box 만을 predict한다.  
그러나 몇몇 큰 객체나 여러 cell의 경계 근처에 있는 객체는 여러 cell에 의해서 잘 찾아질 수 있다.  
Non-Maximal Suppression은 이런 다중 detection을 고치는 데 사용할 수 있다.  
R-CNN이나 DPM와는 다르게 성능에 있어서 결정적이지는 않는다.  
NMS는 mAP에 2~3%의 향상을 가져온다.

## YOLO의 Limitations

YOLO는 각 격자 cell이 오직 2개의 box와 하나의 class만을 예측하기 때문에,  
bounding box prediction에 있어서 큰 공간적인 제약이 있다.  
이 공간적 제약은 모델이 예측할 수 있는 주변 객체의 수를 제한한다.  
모델은 새 떼와 같은 그룹 안에 나타난 작은 객체와 같은 것에 어려움을 겪는다. 

데이터로부터 bounding box를 예측하는 것을 학습하기 때문에, 새롭거나 흔치 않은 종횡비나 설정의 객체를 일반화하는 것에 어려움을 겪는다.  
모델의 구조 상, 입력 이미지부터 다수의 downsampling layer가 있기 때문에  
모델은 bounding box를 예측하는 데 상대적으로 조악한(coarse) feature를 사용한다.  

detection 성능을 추정하는 loss function에 학습을 진행하는 동안,  
loss function은 큰 bounding box와 작은 bounding box의 error를 동일하게 다룬다.  
큰 box의 작은 error는 일반적으로 무해하지만, 작은 box의 작은 error는 IOU에 있어서 더 큰 효과를 가진다.  
우리 error의 주 원인은 부정확한 localization에 있다.



