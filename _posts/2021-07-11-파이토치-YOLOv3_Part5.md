---
title: "YOLO v3  Pytorch로 바닥부터 구현해보기 part.4"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
  - YOLO
  - Non-Maximum Suppression
  - Confidence Thresholding
use_math: true
---

이 글은 YOLO v3 detector를 바닥부터 구현하는 튜토리얼의 5번째 part이다.  
이전 파트에서는 신경망의 출력을 detection prediction으로 변환하는 함수를 구현했다.  
이제는 입력과 출력 파이프라인을 만드는 것 만이 남아있다.

이 튜토리얼은 5개의 파트로 나누어져 있다.  

Part 1 : [YOLO가 어떻게 작동하는지 이해하기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part1)  
Part 2 : [YOLO의 신경망 구조의 layer들을 만들기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part2/)  
Part 3 : [신경망의 순전파 과정을 구현하기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part3/)  
Part 4 : [Objectness Score Thresholding and Non-Maximum Suppression](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part4/)  
Part 5 : 입/출력 파이프라인을 설계하기   

이번 파트에서는 detector의 입력과 출력 파이프라인을 만들 것이다.  
디스크에서 이미지를 읽고, prediction을 하고, prediction을 이용하여 bounding box를 그려낸다.  
그리고 bounding box가 그려진 이미지를 디스크에 저장한다.  

또한 비디오나 real-time으로 들어오는 카메라 이미지에 detector가 작동하는 방법을 알아본다.  
그리고 신경망의 다양한 하이퍼 파라미터에 대한 몇몇 experimentaion을 허용하도록 하는  몇 개의command line flag를 도입해 볼 것이다.

***(이번 파트에서는 OpenCV 3를 설치해야 합니다.)***

지금까지 만들어 온 detector의 경로에 `detector.py` 파일을 만들어주고,  
해당 파일의 상단에 아래와 같은 라이브러리들을 import 해준다.

```python
from __future__ import division
import time
import torch 
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
import cv2 
from util import *
import argparse
import os 
import os.path as osp
from darknet import Darknet
import pickle as pkl
import pandas as pd
import random
```

### Command Line 속성 생성하기

`detector.py`가 지금까지 만든 detector를 실행시키게 될 파일이기 때문에,  
실행시켰을 때 전달할 수 있는 Command line 속성을 만드는 것이 좋을 것이다.  
파이썬의 `ArgParse` 모듈을 이용하여 위의 것들을 해보자.

```python
def arg_parse():
    """
    detect module에 속성들을 파싱해준다.
    
    """
    
    parser = argparse.ArgumentParser(description='YOLO v3 Detection Module')
   
    parser.add_argument("--images", dest = 'images', help = 
                        "Image / Directory containing images to perform detection upon",
                        default = "imgs", type = str)
    parser.add_argument("--det", dest = 'det', help = 
                        "Image / Directory to store detections to",
                        default = "det", type = str)
    parser.add_argument("--bs", dest = "bs", help = "Batch size", default = 1)
    parser.add_argument("--confidence", dest = "confidence", help = "Object Confidence to filter predictions", default = 0.5)
    parser.add_argument("--nms_thresh", dest = "nms_thresh", help = "NMS Threshhold", default = 0.4)
    parser.add_argument("--cfg", dest = 'cfgfile', help = 
                        "Config file",
                        default = "cfg/yolov3.cfg", type = str)
    parser.add_argument("--weights", dest = 'weightsfile', help = 
                        "weightsfile",
                        default = "yolov3.weights", type = str)
    parser.add_argument("--reso", dest = 'reso', help = 
                        "Input resolution of the network. Increase to increase accuracy. Decrease to increase speed",
                        default = "416", type = str)
    
    return parser.parse_args()
    
args = arg_parse()
images = args.images
batch_size = int(args.bs)
confidence = float(args.confidence)
nms_thesh = float(args.nms_thresh)
start = 0
CUDA = torch.cuda.is_available()
```

위의 것들 중에서, 중요한 flag들은 `images` (입력 이미지나 그 경로를 특정하는데에 사용),  
`det` (detection을 저장할 경로), `reso` (입력 이미지의 해상도, speed-accuracy tradeoff에 사용될 수 있음), `cfg` (선택할 수 있는 configuration file) 그리고 `weightfile` 이다.

### 신경망 불러오기

[여기](https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names)에서 `coco.name` 파일을 다운로드한다. 파일은 COCO dataset 안의 객체 이름들을 포함하고 있다.  
detector 경로 안에 `data` 폴더를 만든다.  
그리고 프로그램 안에 class 파일을 불러온다.

```python
num_classes = 80    # For COCO
classes = load_classes("data/coco.names")
```

`load_classes`는 `util.py` 안에 정의된 함수로, 모든 클래스의 인덱스에서 그 클래스 이름의 스트링으로 매핑해주는 dictionary를 반환해준다.

```python
def load_classes(namesfile):
    fp = open(namesfile, "r")
    names = fp.read().split("\n")[:-1]
    return names
```

신경망을 초기화하고, 가중치를 적재한다.

```python
# 신경망을 준비
print("Loading network.....")
model = Darknet(args.cfgfile)
model.load_weights(args.weightsfile)
print("Network successfully loaded")

model.net_info["height"] = args.reso
inp_dim = int(model.net_info["height"])
assert inp_dim % 32 == 0 
assert inp_dim > 32

# GPU가 사용 가능하면, 모델을 GPU에 놓는다.
if CUDA:
    model.cuda()

# 모델을 평가(eval) 모드로 만든다.
model.eval()
```

### 입력 이미지를 읽어오기

디스크로부터 이미지를 읽거나, 이미지가 담긴 폴더 경로를 읽어온다.  
이미지(들)의 경로(들)은 `imlist`라는 list에 저장된다. 

```python
read_dir = time.time()
# Detection phase
try:
    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]
except NotADirectoryError:
    imlist = []
    imlist.append(osp.join(osp.realpath('.'), images))
except FileNotFoundError:
    print ("No file or directory with the name {}".format(images))
    exit()
```

`read_dir`은 시간을 측정하기 위해 사용되는 체크포인트이다. (이 것을 여러 번 만나게 될 것이다.)  

만약 detection을 저장하기 위한 경로, `det` flag가 존재하지 않으면, 만들어 낸다.

```python
if not os.path.exists(args.det):
    os.makedirs(args.det)
```

이미지를 불러오기 위해 OpenCV를 사용할 것이다.

```python
load_batch = time.time()
loaded_ims = [cv2.imread(x) for x in imlist]
```

`load_batch`는 또 다시 체크포인트다.

OpenCV는 이미지를 BGR 순의 컬러 채널을 가진 numpy 배열로 불러온다.  
Pytorch의 입력 이미지 포맷은 **(Batches x Channels x Height x Width)**로, 채널의 순서는 RGB의 순이 되어야 한다.  
그러므로 `util.py` 안에 `prep_image` 함수를 작성해 numpy 배열을 Pytorch 입력 포맷으로 변환한다.  

이 함수를 작성하기 전에, 이미지를 리사이즈하고, 종횡비가 항상 일정하도록 유지하도록 하고, 남겨진 영역을 (128, 128, 128)로 패딩해주는 `letterbox_image` 함수를 먼저 작성해야 한다.

```python
def letterbox_image(img, inp_dim):
    '''패딩을 사용하여 종횡비가 변하지 않도록 이미지를 리사이즈'''
    img_w, img_h = img.shape[1], img.shape[0]
    w, h = inp_dim
    new_w = int(img_w * min(w/img_w, h/img_h))
    new_h = int(img_h * min(w/img_w, h/img_h))
    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)
    
    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)

    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image
    
    return canvas
```

그러고 난 뒤에, OpenCV 이미지를 받고 신경망의 입력으로 변환하는 함수를 작성할 수 있다.

```python
def prep_image(img, inp_dim):
    """
    신경망에 입력하기 위한 이미지 준비 
    
    torch.Variable로 반환
    """

    img = cv2.resize(img, (inp_dim, inp_dim))
    # ::-1로 BGR -> RGB 불러오고 h x w x c -> c x h x w로 변환
    img = img[:,:,::-1].transpose((2,0,1)).copy()
    img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)
    return img
```

변형된 이미지에 추가로, 원래의 이미지에 대한 list도 유지할 것이다.  
`im_dim_list`가 원본 이미지의 차원을 포함하고 있는 list이다.

```python
# 이미지에 대한 PyTorch Variables
im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))

# 원본 이미지 차원을 포함하는 list
im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]
im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)

if CUDA:
    im_dim_list = im_dim_list.cuda()
```

### 배치 만들기

```python
leftover = 0
# 원본 이미지 수에 배치 크기를 나눴을 때, 나머지가 1인 경우
if (len(im_dim_list) % batch_size):
   leftover = 1

# 이미지를 배치 크기만큼 나누어 concatenate
if batch_size != 1:
   num_batches = len(imlist) // batch_size + leftover            
   im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,
                       len(im_batches))]))  for i in range(num_batches)]  
```

### Detection 루프 돌리기

배치를 반복하면서, prediction을 만들고, 위에서 detection했던 모든 이미지의 (D x 8 크기의 `write_results` 함수의 출력) prediction tensor를 concatenate한다.

각 배치에 대해서, 입력부터 `write_results` 함수의 출력까지 detection이 만들어 지는데 소요된 시간을 측정한다.  
`write_prediction`에 반환된 출력에서, 속성들 중 하나가 배치 안에서 이미지의 인덱스를 의미한다.  
이런 특정한 속성을 모든 이미지의 경로를 포함하고 있는 list인 `imlist` 안의 이미지 인덱스를 나타내도록 변환할 것이다.

그런 다음, 각 이미지에서 객체가 detect 되는 시간 뿐만 아니라, 각 detection이 걸린 시간을 출력할 것이다.

만약 배치에서의 `write_results` 함수의 출력이 `int(0)`이면, detection된 것이 없다는 것이다.   
이 경우 `continue`를 사용하여 나머지 루프로 건너 뛴다.

```python
write = 0
start_det_loop = time.time()
for i, batch in enumerate(im_batches):
    #load the image 
    start = time.time()
    if CUDA:
        batch = batch.cuda()

    prediction = model(Variable(batch, volatile = True), CUDA)

    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)

    end = time.time()

    if type(prediction) == int:

        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):
            im_id = i*batch_size + im_num
            print("{0:20s} predicted in {1:6.3f} seconds".format(image.split("/")[-1], (end - start)/batch_size))
            print("{0:20s} {1:s}".format("Objects Detected:", ""))
            print("----------------------------------------------------------")
        continue

    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist 

    if not write:                      #If we have't initialised output
        output = prediction  
        write = 1
    else:
        output = torch.cat((output,prediction))

    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):
        im_id = i*batch_size + im_num
        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]
        print("{0:20s} predicted in {1:6.3f} seconds".format(image.split("/")[-1], (end - start)/batch_size))
        print("{0:20s} {1:s}".format("Objects Detected:", " ".join(objs)))
        print("----------------------------------------------------------")

    if CUDA:
        torch.cuda.synchronize()       
```

`torch.cuda.synchronize`는 CUDA 커널이 CPU와 동기화되도록 만든다.   
그렇지 않는 경우 CUDA 커널은 GPU 작업이 queue되거나, GPU의 작업이 완료되기 전에 CPU에 권한을 돌려준다. (Asynchronous calling)  
이 것은 GPU의 작업이 실제로 끝나기도 전에 `end = time.time()`이 출력된다면, 잘못 측정된 시간이 나올 수 있다.

출력 텐서의 모든 이미지에 대한 detection을 가지게 되었다.  
이제 이미지에 bounding box를 그려보자.

### 이미지에 bounding box를 그리기

try-except 블록을 통해 하나의 detection이라도 됐는지 확인하였다.  
하나도 detection이 되지 않았다면, 프로그램을 종료한다.

```python
try:
    output
except NameError:
    print ("No detections were made")
    exit()
```

bounding box를 그리기 전에, 출력 텐서에 포홤되어 있는 prediction을 원본 이미지 사이즈가 아니라 신경망의 입력 사이즈로 맞춰야 한다.  
그래서 bounding box를 그리기 전에, 각 bounding box의 좌표쌍 속성을 이미지의 원본 차원으로 변환해야 한다.

bounding box를 그리기 전, 출력 텐서에 포함된 prediction은 원본 이미지가 아닌 패딩된 이미지에 대한 prediction이다.  
단지 입력 이미지의 차원으로 re-scaling하는 것은 되지 않을 것이다.  
먼저 원본 이미지를 포함하는 패딩된 이미지의 영역 경계 측면을 측정할 box의 좌표를 변환할 필요가 있다.

```python
im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())

scaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)


output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2
output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2
```

그러면 좌표는 패딩된 영역의 이미지 차원에 맞춰진다.  
그러나 `letterbox_image` 함수 안에서 scaling factor로 이미지의 두 차원을 리사이즈 했다.  
(두 차원 모두 종횡비를 유지하기 위해 공통 인수로 나누어 졌다는 것을 기억하자.)  
원본 이미지의 bounding box 좌표를 구하기 위해 위의 rescaling을 되돌리자.

```python
output[:,1:5] /= scaling_factor
```

이미지의 바깥에 경계를 가지고 있을 수 있는 bounding box에 대해 이미지의 끝으로 clip하자.

```
for i in range(output.shape[0]):
    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])
    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])
```

만약 이미지에 너무 많은 bounding box가 있다면, 모든 bounding box를 하나의 색으로 그리는 것은 그리 좋은 생각은 아니다.  
detector 폴더 안에 [이 파일](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/raw/master/pallete)을 다운로드하자.  
이 것은 랜덤하게 선택된 많은 색상들을 포함하고 있는 pickle file이다.

```python
class_load = time.time()
colors = pkl.load(open("pallete", "rb"))
```

그럼 box를 그리는 함수를 작성하자.

```python
draw = time.time()

def write(x, results, color):
    c1 = tuple(x[1:3].int())
    c2 = tuple(x[3:5].int())
    img = results[int(x[0])]
    cls = int(x[-1])
    label = "{0}".format(classes[cls])
    cv2.rectangle(img, c1, c2,color, 1)
    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]
    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4
    cv2.rectangle(img, c1, c2,color, -1)
    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);
    return img
```

위의 함수는 `colors`로부터 랜덤하게 선택된 색상의 사각형을 그린다.  
또한 bounding box의 좌상단에 색이 채워진 사각형을 만들고, 그 사각형을 통해 탐지된 객체의 클래스를 적는다.   
`cv2.rectangle` 함수의 `-1` 인자는 채워진 사각형을 만드는데 사용된다.  

지역적으로 `write` 함수를 정의하여 `colors` list에 접근할 수 있도록 한다.  
`colors`를 인자로 포함시킬 수 있지만, 이미지 당 하나의 색상만을 사용하도록 하기 때문에 원래의 목적에는 부합하지 않는다.

일단 이 함수를 정의하고 나면, 이미지에 bounding box를 그릴 수 있게 된다.

```python
list(map(lambda x: write(x, loaded_ims), output))
```

위의 코드는 `loaded_ims` 안의 이미지에 바로 bbox를 적용시켜 list로 저장한다.

각 이미지는 이미지 이름 앞에 `det_`이라는 접두사가 붙여져 저장된다.  
그리고 detection된 이미지가 저장될 경로의 리스트를 만든다.

```python
det_names = pd.Series(imlist).apply(lambda x: "{}/det_{}".format(args.det,x.split("/")[-1]))
```

마지막으로 `det_names` 안의 경로에 detection된 이미지를 저장한다.

```python
list(map(cv2.imwrite, det_names, loaded_ims))
end = time.time()
```

### 시간 요약 출력하기

detector의 맨 마지막에 어느 부분의 코드가 실행되는데 오래 걸렸는지를 포함하고 있는 요약을 출력할 것이다.  
이는 다른 하이퍼파라미터가 detector의 속도에 어떻게 영향일 주는지 비교할 때 도움이 된다.  
배치 사이즈 (`bs`), objectness confidence (`confidence`), NMS threshold (`nms_thresh`)와 같은 하이퍼 파라미터는 command line에서 `detection.py`가 실행될 때 설정될 수 있다.

```python
print("SUMMARY")
print("----------------------------------------------------------")
print("{:25s}: {}".format("Task", "Time Taken (in seconds)"))
print()
print("{:25s}: {:2.3f}".format("Reading addresses", load_batch - read_dir))
print("{:25s}: {:2.3f}".format("Loading batch", start_det_loop - load_batch))
print("{:25s}: {:2.3f}".format("Detection (" + str(len(imlist)) +  " images)", output_recast - start_det_loop))
print("{:25s}: {:2.3f}".format("Output Processing", class_load - output_recast))
print("{:25s}: {:2.3f}".format("Drawing Boxes", end - draw))
print("{:25s}: {:2.3f}".format("Average time_per_img", (end - load_batch)/len(imlist)))
print("----------------------------------------------------------")


torch.cuda.empty_cache()
```

### Object Detector 테스트하기.

예를 들어, 터미널에서 아래와 같이 적어 실행시키면,

```
python detect.py --images dog-cycle-car.png --det det
```

출력을 만들어 낸다.

```
Loading network.....
Network successfully loaded
dog-cycle-car.png    predicted in  2.456 seconds
Objects Detected:    bicycle truck dog
----------------------------------------------------------
SUMMARY
----------------------------------------------------------
Task                     : Time Taken (in seconds)

Reading addresses        : 0.002
Loading batch            : 0.120
Detection (1 images)     : 2.457
Output Processing        : 0.002
Drawing Boxes            : 0.076
Average time_per_img     : 2.657
----------------------------------------------------------
```

`det_dog-cycle-car.png`의 이름의 이미지가 `det` 경로에 저장되었다.

### 비디오/웹캠에서 Detector 실행시키기.

비디오나 웹캠에서 detector를 작동시키기 위해서,  
대부분의 코드는 같지만, 이미지 배치가 아니라 비디오의 프레임을 반복해야한다.

비디오에 detector를 실행시키기 위한 코드는 원본 Github repository의 `video.py` 파일 안에서 찾아볼 수 있다.  
`detect.py`와 몇 가지 변화 외에는 동일하다.

먼저, 비디오를 열거나 OpenCV를 통해 카메라 피드를 받는다.

```python
videofile = "video.avi" #or path to the video file. 

cap = cv2.VideoCapture(videofile)  

#cap = cv2.VideoCapture(0)  for webcam

assert cap.isOpened(), 'Cannot capture source'

frames = 0
```

프레임을 반복하는 것은 이미지들을 반복하는 것과 비슷한 방식이다.

더 이상 배치를 다루지 않고 한번에 한 이미지 만을 다루기 때문에 많은 양의 코드가 여러 곳에서 단순화되었다.
이 것이 한 프레임 한번에 하나의 이미지만 올 수 밖에 없기 때문이다.  
`im_dim_list`를 사용하는 대신에 튜플을 사용하게 되었고, `write` 함수에 약간의 변화가 생겼다.

매 반복마다, `frames`라는 변수 안에 캡쳐된 프레임의 수를 저장한다.  
이 수를 첫 프레임으로부터 경과된 시간으로 나누어 비디오의 FPS를 출력한다.

`cv2.imwrite`를 사용해 detect된 이미지를 디스크로 저장하는 대신에,  
`cv2.imshow`를 사용하여 bounding box가 그려진 프레임을 보여준다.  
만약 사용자가 `Q` 버튼을 누르면, 루프의 `break`가 발생하고 비디오가 끝나도록 한다.

```python
frames = 0  
start = time.time()

while cap.isOpened():
    ret, frame = cap.read()
    
    if ret:   
        img = prep_image(frame, inp_dim)
#        cv2.imshow("a", frame)
        im_dim = frame.shape[1], frame.shape[0]
        im_dim = torch.FloatTensor(im_dim).repeat(1,2)   
                     
        if CUDA:
            im_dim = im_dim.cuda()
            img = img.cuda()

        output = model(Variable(img, volatile = True), CUDA)
        output = write_results(output, confidence, num_classes, nms_conf = nms_thesh)


        if type(output) == int:
            frames += 1
            print("FPS of the video is {:5.4f}".format( frames / (time.time() - start)))
            cv2.imshow("frame", frame)
            key = cv2.waitKey(1)
            if key & 0xFF == ord('q'):
                break
            continue
        output[:,1:5] = torch.clamp(output[:,1:5], 0.0, float(inp_dim))

        im_dim = im_dim.repeat(output.size(0), 1)/inp_dim
        output[:,1:5] *= im_dim

        classes = load_classes('data/coco.names')
        colors = pkl.load(open("pallete", "rb"))

        list(map(lambda x: write(x, frame), output))
        
        cv2.imshow("frame", frame)
        key = cv2.waitKey(1)
        if key & 0xFF == ord('q'):
            break
        frames += 1
        print(time.time() - start)
        print("FPS of the video is {:5.2f}".format( frames / (time.time() - start)))
    else:
        break     
```

### 결론

이 튜토리얼을 통해, 바닥부터 object detector를 구현했다.  

*개인적인 소감은 이 튜토리얼을 번역하면서, YOLO의 기반이 되는 알고리즘에 대해 더 찾아보고 공부할 수 있게 되었다.  
물론 100%를 이해할 수는 없었지만, 다음 글부터는 YOLO의 제일 처음 v1부터 논문을 리뷰하며, 정리해볼 생각이다.  
이전에 Tensorflow Object Detection API를 다뤄 객체 탐지 프로젝트를 했던 적이 있는데,  
그 때 시간 관계상 API를 사용할 수 밖에 없었고, 더 깊은 공부를 하지 못했다는 점에서 아쉬움이 남았다.  
같은 One-stage 방식의 SSD의 논문도 리뷰해보고, 다른 점을 고찰해야겠다.  
그 이후, Two-stage 방식의 여러 좋은 알고리즘들이 많으니 더 공부해보자!*

