---
title: "YOLO v3, Pytorch로 바닥부터 구현해보기 part.1"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
use_math: true
---

### YOLO v3, Pytorch로 바닥부터 구현해보기

우선 이 글은 아래의 출처의 튜토리얼을 번역한 것임을 밝힙니다.  

https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/  

이 튜토리얼은 총 5개의 파트로 구성되어 있으며, 목차는 아래와 같습니다.  

Part 1. YOLO가 어떻게 작동하는지 이해하기.  
Part 2. YOLO의 신경망 구조의 layer들을 만들기.  
Part 3. 신경망의 순전파 과정을 구현하기.  
Part 4. Objectness score thresholding and Non-maximum suppression.  
Part 5. 입/출력 파이프라인을 설계하기.  

#### YOLO?

YOLO는 You Only Look Once의 약자로,  
깊은 수준의 합성곱 신경망을 통해 학습한 특징들을 사용하는 객체 탐지기이다.  
먼저 Pytorch로 YOLO를 구현하기 전에, 이것이 어떻게 작동하는지 알아야 한다.  

#### FCNN, Fully Convolutional Neural Network

YOLO는 합성곱 레이어들로만 이루어져있다.  
이러한 신경망을 Fully Convolutional Network, FCN이라고 한다.  
YOLO는 75개의 convolution layers와 함께, skip connections와 upsampling layers들도 포함한다.  
일반적인 CNN와는 다르게 Pooling layer는 사용되지 않으며,  
stride 2의 convolution layer가 feature map을 downsampling하는 데에 사용된다.  
이 것은 Pooling으로 인해 자주 발생되는 low-level의 feature의 손실을 막는데 도움이 된다.  

YOLO가 FCN이기 때문에, 입력 이미지의 사이즈에는 영향을 받지 않는다.  
그러나, 실제에서는 알고리즘을 구현할 때 발생하는 여러 가지 문제들로 인해서  
일관된 입력 사이즈를 받는 것이 바람직하다.  

일관되지 않은 입력 사이즈로 인해 발생되는 문제들 중 가장 큰 것은 이미지들을 배치에서 작업하길 원할 때에 발생한다.  
(배치에서의 이미지들은 GPU에 의해 병렬적으로 작업될 수 있으며, 이는 속도가 더 빨라지는 결과를 가져온다.)  
이미지들을 배치에 넣어 놓고 작업하기 위해서는, 모든 이미지들의 height와 width가 동일하게 고정되어 있어야 한다.  
이를 통해 다수의 이미지들을 하나의 큰 배치로 concatenate 할 수 있다. (많은 이미지 tensor를 하나의 tensor로 병합)  

신경망은 stride를 통해 이미지를 downsampling한다.  
예를 들어, stride가 32이고 입력 이미지의 크기가 416x416이면, 13x13 크기의 결과가 만들어 질 것이다.  

#### output 이해하기.

전형적으로, convolution layer에 의해 학습된 feature들은 탐지 예측을 하는 classifier / regressor에 전달된다.  
(bounding boxes의 위치 좌표를 만들거나, class label을 예측하거나 등등...)  
(*아마 two-stage detector에 대한 이야기인 것 같음.)  

YOLO에서는 1x1 convolution layer를 통하여 예측을 완료한다.  

그래서 주목해야 할 첫 번째는 우리의 output이 feature map이라는 것이다.  
1x1 convolution을 사용했기 때문에, prediction map의 크기는 이전의 feature map의 크기와 정확하게 일치한다.  
YOLO v3 (그 이후의 버전들)에서, 이 prediction map을 이해하기 위한 방법은 각각의 cell이 고정된 수의 bounding boxes를 예측할 수 있다는 것이다.  

깊이 방향으로 feature map에서 $(B \times (5 + C))$개의 엔트리가 있다.  
$B$는 각 cell에서 예측할 수 있는 bounding box의 수를 나타낸다.  
YOLO 논문에 따르면, 각각의 이런 bounding box B는 특정 종류의 객체를 탐지하는데 특화될 수 있다고 말한다.  
각 bounding box들은 $5 + C$개의 attribute가 있는데,  
이는 객체의 중앙 좌표, 차원, objectness score, 각 bounding box에 대한 C class confidences에 대한 것이다.  
YOLO v3에서는 모든 cell에서 3개의 bounding box를 예측한다.  

feature map의 각 cell들은 해당 cell의 수용장에 객체의 중앙이 맞아 떨어지면,  
그 cell의 bounding box들을 통해서 객체를 예측한다.  

이 점은 YOLO가 학습하는 방법과 관련이 있다. 오직 하나의 bounding box가 주어진 객체를 탐지한다.  
먼저 이 bounding box가 어느 cell에 속할지에 대한 것을 명확하게 해야한다.  

그 것을 위해서, 입력 이미지를 최종 feature map과 동일한 차윈의 격자로 나눈다.  
이해를 위해서 아래의 예제를 보자.

![](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolo-5.png?raw=true)

예를 들어, 입력 이미지의 크기는 416x416이고, 신경망의 stride는 32라고 해보자.  
앞서 언급했듯이, feature map의 차원은 13x13이 될 것이다.  
따라서 입력 이미지를 13x13 크기의 cell들로 나누어보자.  

그런 다음, 입력 이미지에서 객체의 ground truth box의 중앙을 포함하고 있는 cell은 객체를 예측하기 위한 것으로 선택된다.  
위의 이미지에서 붉은 색으로 표시된 cell이며, 노란 색으로 표시된 ground truth box의 중앙을 포함하고 있다.  

보면 붉은 색 cell은 격자에서 7번째 행의 7번째 cell이다.  
feature map에서 대응되는 (7, 7) 위치의 cell을 객체를 탐지해야 할 cell로 배정한다.  

이 cell은 3개의 bounding box들을 예측할 수 있다.  
3개 중 어느 것이 ground truth label에 할당될까?  
이 것을 이해하기 위해서, anchor의 개념에 대해 이해해야만 한다.  

#### Anchor Boxes

bounding box의 높이와 넓이를 예측하는 것은 말이 될지 모르겠지만,  
그러나 실제로 그것은 학습하는 동안 불안정한 gradient를 불러온다.  
대신에 대부분의 현대 객체 탐지기들은 log-space transform으로 예측하거나  
anchor라 불리는 미리 정의된 기본 bounding box를 통해 offset한다.  

이 변형들은 예측을 하기 위해 anchor box들에 적용된다.  
YOLO v3는 3개의 anchor를 가지고 있으며, 이는 cell 당 3개의 bounding box의 예측을 낸다.  

앞의 질문으로 돌아와서, 객체를 탐지해야 할 bounding box는  
ground truth box에 대해 가장 높은 IOU(Intersection Over Union) 값을 지닌 anchor가 될 것이다.  

#### 예측하기

아래의 식이 어떻게 신경망의 결과가 bounding box 예측을 하기 위해 변형되었는지 설명해준다.  

<p>
$$\begin{align}
   b_x & = \sigma(t_x) + c_x \\
   b_y & = \sigma(t_y) + c_y \\
   b_w & = p_{w}e^{t_w} \\
   b_h & = p_{h}e^{t_h} \\
\end{align}$$
</p>

$b_x$, $b_y$, $b_w$, $b_h$는 x,y의 중심 좌표와 예측의 높이와 넓이이다.  
$t_x$, $t_y$, $t_w$, $t_h$는 신경망의 결과값이다.  
$c_x$, $c_y$는 격자의 좌측-상단(top-left) 좌표이다.  
$p_w$, $p_h$는 box의 anchor 차원이다.  

##### 중심 좌표 값

중심 좌표의 예측을 시그모이드 함수를 통해 수행한다는 사실을 알 수 있다.  
이 것은 결과 값을 0에서 1 사이로 제한하는데, 왜 이렇게 해야 하는 것일까?  

일반적으로 YOLO는 bounding box 중심의 절대적인 좌표값을 예측하지 않는다.  
아래에 해당하는 offsets을 예측한다.  

- 객체를 예측하는 cell의 좌-상단의 모서리  
- feature map에서의 cell의 차원에 의해 정규화된 값, 즉 1  

예를 들어, 위에서의 개 이미지에 대해 생각해보자.  
만약 예측한 중앙 좌표가 (0.4, 0.7)이라면, 이는 13 x 13 feature map에서 중앙 좌표가 (6.4, 6.7)에 놓여있다는 것을 의미한다.  
(붉은 색의 좌-상단의 좌표가 (6, 6)이기 때문에)  

그렇다면 만약 예측된 x,y 좌표가 (1.2, 0.7)이라면 어떻게 되는 것일까?  
이는 중앙 좌표가 (7.2, 6.7)에 위치한다는 것이다.  
이렇게 되면, 중앙은 붉은 색 cell의 바로 옆에 놓여지게 되거나 (7, 6), 7번째 행의 8번째 cell(8, 7)이 될 수 있다.  
이는 YOLO의 기반 이론을 위반하는 것이다.  
왜냐하면 만약 붉은 색 cell을 객체를 예측하기 위한 것으로 상정했다면 객체의 중앙은 다른 곳이 아닌 붉은 색 cell의 위치에 놓여져야 한다.  

따라서 이러한 문제점의 해결책으로, 신경망의 결과값은 시그모이드 함수를 통과하게 된다.  
결과값의 범위를 0에서 1로 줄여, 격자에서 예측하고 있는 중앙 좌표 값을 효과적으로 유지할 수 있다.  

#### Bounding Box의 차원

Bounding Box의 차원들은 결과에 log-space transform을 적용되어 예측되고, 그 다음 anchor에 곱해진다.  

![http://christopher5106.github.io/](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolo-regression-1.png?raw=true)

탐지기의 결과가 어떻게 변화되어 최종 예측을 하는지에 대한 설명이 위의 그림으로 표현되어 있다.  

예측된 결과, $b_w$와 $b_h$는 이미지의 높이와 넓이에 의해 일반화된다. (학습 label들은 이 방식으로 선택됨.)  
그래서 만약 객체를 포함하는 box에 대한 예측된 $b_x$와 $b_y$가 (0.3, 0.8)이라면,  
13x13 feature map에서 실제 넓이와 높이는 (13 x 0.3, 13 x 0.8) = (3.9, 10.4) 이다.  

#### Objectness Score

Objectness Score는 bounding box 안에 객체가 포함될 가능성을 나타낸다.  
붉은 색 cell과 그 이웃한 격자에서 1에 가깝게 되고, 격자가 모서리에 존재하게 되면 0에 가깝게 된다.  

Objectness Score 또한 시그모이드를 통과하기 때문에, 확룰로 해석될 수 있다.  

#### Class Confidence

Class Confidence는 탐지된 객체가 특정 class에 속할 확률을 나타낸다.  
YOLO v3 이전의 모델에서는 Class scoring에 소프트맥스를 사용했었다.  

그러나 소프트맥스를 사용했던 것은 v3에서는 선택되지 않았고, 대신 시그모이드를 사용하는 것으로 바뀌었다.  
그 이유는 소프트맥스 class score가 클래스들이 상호배타적이라고 가정하기 때문이다.  
간단히 말하자면, 만약 객체가 하나의 클래스에 속한다면 그것은 다른 클래스에 속하지 못한다는 것을 보장하게 된다.  
COCO database에서는 그런 것이 사실이 된다.  

그러나 이러한 가정은 여성과 사람이라는 클래스를 가지는 때에는 옳지 못하다.  
이것은 YOLO의 저자가 소프트맥스를 사용하지 않도록 방향을 튼 이유가 되었다.  

#### 다른 스케일들을 통한 예측

YOLO v3는 세 가지의 다른 스케일을 통해 예측을 한다.  
탐지 계층은 stride가 32, 16, 8인 3개의 다른 크기를 가진 feature map에서 탐지를 한다.  
이 것은 입력 사이즈가 416x416인 경우, 13x13, 26x26, 52x52 스케일에서 탐지를 한다는 것을 의미한다.  

첫 탐지 계층 이전까지 신경망은 입력 이미지를 downsampling한다.  
첫 탐지는 stride 32인 feature map의 계층을 사용하여 이뤄진다.  
더 나아가 계층들은 2배로 upsampling되고 동일한 크기를 가지는 이전 계층의 feature map과 연결된다.  
다른 탐지는 stride 16인 계층에서 이뤄진다.  
동일한 upsampling 절차가 반복되고, 최종적인 탐지는 stride 8인 계층에서 이뤄진다.  

각각의 스케일에서, 각 cell은 3개의 anchor를 사용하여 3개의 bounding box을 예측해낸다.  
총 9개의 anchor가 만들어지고 사용된다. (anchor들은 다른 스케일마다 다르다.)  

![](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolo_Scales-1.png?raw=true)

저자는 이 방식이 YOLO v3가 이전 버전에서 자주 문제가 되었던 작은 객체를 탐지하는 것에 있어서 도음을 준다고 보고했다.  
upsampling이 작은 객체를 탐지하는 것에 중요한 세부적인 feature들을 신경망이 학습하는데 도움을 준다.  

### 결과 처리

크기가 416x416인 이미지에 대해  
YOLO는 $((52 \times 52) + (26 \times 26) + (13 \times 13) \times 3) = 10641$개의 bounding box들을 예측한다.  
그러나 위의 이미지의 경우 단 하나의 객체, 개만 존재할 뿐이다.  
어떻게 10647개에서 1개로 줄여서 예측해낼 수 있을까?  

#### Object Confidence에 대한 Thresholding

우선 box들의 Objectness Score를 기반으로 필터링한다.  
일반적으로 threshold보다 낮은 score를 가지고 있는 box들은 무시된다.  

#### Non-Maximum Suppression

Non-Maximum Suppression(NMS)는 같은 이미지에 대한 다수의 탐지 문제를 해결하기 위해 고안되었다.  
예를 들어, 모든 3개의 bounding box의 붉은 색 cell이 box를 탐지하거나, 인접한 cell이 같은 객체를 탐지할 수도 있다.  
NMS는 아래와 같은 단계를 거치게 된다.  

1. 동일한 class를 예측한 bounding box(bb)를 Confidence Score의 내림차순으로 정렬한다.  
2. 첫 bb와 그 다음 bb의 IOU 값이 threshold를 넘으면, 후자의 bb를 제거한다.  
3. 2의 단계를 반복하다 넘지 않는 bb가 존재하면, 해당 bb는 건너 뛴다.  
4. 모든 bb를 훑었다면, 그 다음으로 큰 Score의 bb를 선택하여, 2단계로 돌아간다.  

즉 confidence score가 높은 순으로 순차적으로 IOU를 비교하면서,  
IOU가 높은 경우에는 동일한 객체를 탐지했다고 판단하여 해당 bb는 삭제하는 방식이다.  

전체적으로는  
1. Confidence Score를 Threshold 값을 통해 필터링  
2. 각 Class의 Bounding Box list를 Confidence Score 기준 내림차순 정렬  
3. NMS 시행  

이를 통해서 많은 수의 bounding box를 줄일 수 있다.  

