---
title: "YOLO v2 (YOLO9000) 논문 리뷰"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
  - YOLO
  - YOLOv1
use_math: true
---

이 글은 [YOLOv2 논문 (YOLO9000: Better, Faster, Stronger)](https://arxiv.org/pdf/1612.08242.pdf)을 처음부터 리뷰합니다.  

이전 YOLOv1과는 다르게, YOLO9000인 이유는 9000개 이상의 객체 범주를 탐지할 수 있어서라고 합니다.

이전 모델에 비해서, YOLOv2는 VOC 2007 dataset에서 67 FPS에 76.8 mAP를 달성했으며, 40 FPS에 78.6 mAP를 달성했습니다.  
이는 Faster R-CNN과 같은 최신의 방법을 뛰어넘는 성능입니다.

이 논문에서는 object detection과 classification을 동시에 학습하는 방법을 제안했습니다.  
YOLO9000은 이 방법을 사용하여, COCO detection dataset과 ImageNet classification dataset을 동시에 학습했습니다.  
이 Joint training (결합 학습?)을 통해서, YOLO9000이 detection data가 라벨링되지 않은 객체 class에 대해 detection을 예측할 수 있도록 만듭니다.

저자들은 이러한 접근을 ImageNet detection task를 통해 평가했고, 
200개의 class 중 44개만이 detection data가 존재했음에도 불구하고, YOLO9000은 19.7 mAP의 성능을 보여주었다고 합니다.
또 156개의 class들이 COCO dataset에는 존재하지 않았지만, YOLO9000dms 16.0 mAP의 성능을 보였다고 합니다.

단지 200개의 class를 탐지할 수 있을 뿐만 아니라 9000개 이상의 다른 객체에 대해 탐지를 예측할 수 있다고 합니다.  
또 그런데도 real-time으로 작동될 수 있다고 합니다.

이제 논문의 내용으로 들어가봅시다.

## 도입

Object Detection의 일반적인 목적은 빠르고, 정확하고 다양한 종류의 객체에 대해 인식할 수 있어야 한다.  
신경망의 도입으로 인해, 큰 속도로 빨라지고 정확해졌지만,  
대부분의 detection 방법들은 작은 객체들을 탐지하는 것에 제약이 있다.

현재의 Object Detection Dataset들은 classification과 tagging과 같은 다른 task에 비해 제한되어 있다.  
가장 일반적인 detection dataset은 수십에서 수백 개의 태그를 가진 수천에서 수십만 개의 이미지가 포함하고 있다.  
반면 Classification dataset은 수만에서 수십만 개의 범주를 가진 수백만 개의 이미지를 가지고 있다.

저자들은 detection dataset의 수준을 object classification dataset의 수준으로 조정하고 싶었다.  
그러나 detection을 위한 이미지 라벨링은 classification이나 tagging을 위한 라벨링보다 훨씬 비싸다.  
(tagging의 경우, 사용자가 공급하기 때문에 무료인 경우가 종종 존재.)  
따라서 가까운 미래에도 classification dataset과 같은 수준과 동일한 detection dataset은 보기 힘들 것이다.  

저자들은 이미 가지고 있는 많은 양의 classification dataset을 현재의 detection system의 시야로 확장시켜 사용하는 새로운 방법을 제안한다.  
이 방법은 object classification의 위계적인 관점을 사용하여 다른 dataset을 섞어 함께 사용할 수 있게 만들어준다.

또한 detection data와 classification data를 object detector에 동시에 학습시킬 수 있도록 하는 joint training algorithm을 제안한다.  
저자들의 방법은 라벨링된 detection 이미지를 학습해 객체를 정확하게 찾는 것을 강화하는 동시에 classification 이미지를 사용하여 vocabulary과 robustness를 증가시킨다.  
(vocabulary : class의 개수인 듯? / robustness : 잘 generalize되어 모델이 흔들리지 않는 강건성)

위의 방법들을 사용하여, YOLO9000을 학습시켰고, 9000개 이상의 다양한 객체 범주를 탐지할 수 있는 real-time object detector이다.  

우선 YOLO를 기반으로 향상시켜 YOLOv2를 만들었으며,  
그 다음 dataset combination method와 joint training algorithm을 사용하여 9000개 이상의 ImageNet data와 COCO의 detection data를 통해 모델을 학습시켰다.

## Better

YOLO는 state-of-the-art(SOTA, 최신의) detection system과 관련된 다양한 단점들로 인해 어려움을 겪었다.  
Fast R-CNN과 비교한 YOLO의 error analysis는 YOLO가 localization error를 유의미하게 만들어 낸다는 것을 보여주었다.  
게다가 YOLO는 region proposal-based method에 비해 상대적으로 낮은 recall을 보인다.  
따라서 저자들은 classification accuracy를 유지하면서 recall과 localization을 향상시키는 것에 중점을 두었다.

Computer vision은 일반적으로 더 크고 깊은 신경망에 트렌드가 흘러가고 있다.  
더 나은 성능은 종종 더 큰 신경망에 학습하거나, 다양한 모델를 함께 앙상블하는 것에 달려있다.  
그러나 YOLOv2에서는 아직 빠르지만, 더 accurate하길 원한다.  
YOLO network를 더 크게 만드는 대신에, network를 간단하게 하고 representation을 학습하기 쉽게 만들었다.  
이전 작업들로부터 YOLO 성능을 향상시키기 위한 새로운 개념에 대한 다양한 아이디어를 모았다.  
그 결과에 대한 요약은 Table 2에서 찾아볼 수 있다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_table2.JPG?raw=true)

### Batch Normalization

Batch Normalization(배치 정규화)은 convergence(수렴)에 있어서 유의미한 향상을 가져옴과 동시에 다른 형태의 regularization(규제)가 필요하지 않게 만든다.  
YOLO의 모든 convolutional network에 batch normalization을 더하는 것은 mAP에 2% 향상을 가져왔음.  
또한 Batch normalization은 모델을 일반화하는데 도움을 준다.  
Batch normalization이 있으면 모델에서 overfitting 없이 dropout을 없앨 수 있다.

### High Resolution Classifier

모든 SOTA Detection method들은 ImageNet으로 pre-train된 classifier를 사용한다.  
AlexNet을 시작으로 대부분의 classfier들은 256 x 256 크기보다 더 작은 입력 이미지를 다룬다.  
원래의 YOLO는 classifier network를 224 x 224 크기로 학습시키고 detection에서는 448 x 448 크기로 리사이즈했다.  
이것은 network가 object detection을 학습하는 것과 새로운 입력 해상도로 조정하는 것을 동시에 바꿔야 한다는 것을 의미한다.

YOLOv2에서는 classification network를 ImageNet data의 완전한 448 x 448 해상도에서 10 epoch만큼 먼저 fine tuning을 했다.  
이는 더 높은 입력 해상도에서 network의 filter가 더 잘 작동되도록 조정하는 시간을 준다.  
그 다음, detection을 위한 최종 network를 fine tuning한다.
이 high resolution classificaiton network는 거의 4%의 mAP 향상을 가져왔다.

### Convolutional With Anchor Boxes.

YOLO는 convolutional feature extractor 위에 fully connected layer들을 직접적으로 사용해 bounding box들의 좌표를 predict한다.  
Faster R-CNN은 직접 좌표를 predict하는 대신에 hand-picked priors(미리 직접 뽑혀진 Anchor box들)을 사용하여 bounding box를 predict한다.  
convolutional layer만 사용하는 Faster R-CNN의 Region Proposal Network(RPN)은 anchor box들에 대한 offset과 confidence를 예측한다.  
prediction layer가 convolutional이기 때문에, RPN은 feature map에서 모든 위치에서의 offset들을 predict 해야한다.  
좌표 대신 offset을 predict하는 것은 문제를 더 간단하게 만들고 network가 학습하기 쉽게 만든다.

YOLO에서 fully connected layer를 제거하고, bounding box를 predict하는 데 anchor box를 사용했다.  
먼저 network의 convolutional layer 출력이 더 높은 해상도로 만들기 위해 하나의 pooling layer를 제거했다.  
또한 448 x 448 크기가 아니라 416 x 416 크기의 입력 이미지를 다루도록 network를 줄였다.   
이것을 왜 했냐면, feature map에 홀수 개의 location이 존재하게 만들어 중앙에 하나의 cell만 존재하도록 원했기 때문이다.  
특히 큰 객체와 같이, 객체들은 이미지의 중앙을 차지하고 있는 경향이 있으므로, 이러한 객체를 predict하기 위해 4개의 location이 모두 주변에 있는 것보다 하나의 location이 바로 중앙에 있는 것이 좋다.  
YOLO의 convolutional layer들은 이미지를 32배 downsampling하고, 416 x 416 크기의 입력 이미지를 사용하면 13 x 13 크기의 feature map 출력을 얻을 수 있다.

anchor box로 바꿀 때 spatial location에서의 class prediction mechanism과 헤어져야 했고,  
대신 모든 anchor box에 대한 class와 objectness를 predict했다.  
YOLO에 따라서, 아직 objectness prediction은 제안된 box와 ground truth의 IOU로 predict하고,  class prediction은 객체가 있는 곳의 해당 class의 조건부 확률로 predict했다.  

anchor box를 사용하는 것은 accuracy에서 약간의 감소를 가져왔다.  
YOLO는 오직 이미지 당 98개의 box들을 예측했지만, anchor box를 사용하는 YOLO 모델은 천개 이상의 box를 예측했다.  
anchor box 없이 사용한 중간 모델은 69.5 mAP와 81%의 recall을 얻었다.  
anchor box를 사용한 모델은 69.2 mAP와 88%의 recall을 기록했다.  
mAP가 감소했더라도, recall에서의 상승은 모델이 더 개선될 여지가 있다는 것을 의미한다.

### Dimension Clusters

YOLO에 anchor box를 적용해 사용할 때, 2개의 문제에 직면하게 되었다.  
첫 번째 문제로는 box dimension이 수동으로 뽑힌 것들이라는 것이라는 것이다.  
network가 적절하게 box를 조절하는 것을 배울 수 있었지만, 만약 network가 가지고 시작할 prior들(anchor box dimensions)을 더 잘 골라 놓으면 network가 좋은 detection을 predict하는 것을 더 쉽게 만들 수 있을 것이다.

손수 prior들을 선택하는 대신, 좋은 prior들을 자동으로 찾기 위해 training set의 bounding box에 대한 k-means clustering을 실시했다.  
만약 유클리디안 거리를 이용한 표준적인 k-means를 사용할 경우, 큰 box들은 작은 box들보다 더 큰 error를 만들어 낼 것이다.  
그러나 저자들이 정말로 원하는 것은 더 좋은 IOU score을 만드는 prior이기 때문에, box의 크기에는 독립적이어야 한다.  
따라서 저자는 아래와 같은 distance metric을 사용하였다.
$$
d(box, centroid) = 1 - IOU(box, centroid)
$$
다양한 $k$ 값을 통해 k-means를 돌렸고, Figure 2에 centroid에 가장 가까운 평균 IOU 값들을 도식화했다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_fig2.JPG?raw=true)

$k = 5$ 인 경우, 모델 복잡도와 높은 recall 사이의 가장 좋은 tradeoff를 보였다.  
cluster centroid는 직접 뽑은 anchor box에 비해 상당한 차이를 보였다.  
짧고 넓은 box들은 더 적었고, 길고 얇은 box들은 더 많았다.

clustering 전략의 가장 가까운 prior와 직접 뽑은 anchor box의 average IOU를 비교해보았다.  
그 결과는 Table 1에서 볼 수 있다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_table1.JPG?raw=true)

5개 centroid를 사용한 prior인 경우 (Avg IOU = 61.0), 9개의 anchor box를 가진 경우 (Avg IOU = 60.9)와 비슷한 성능을 보인다.  
만약 9개의 centroid를 사용한 경우, 훨씬 더 높은 average IOU 값을 보인다.  
bounding box를 생성하기 위해 k-means를 사용하는 것은 모델이 더 나은 representation을 가지고 task를 학습하기 더 쉽게 만든다는 것을 의미한다.

### Direct location prediction

YOLO에 anchor box를 사용할 때 직면한 문제점 중 두 번째 문제이다.  
특히 초기 학습과정에서 더욱 그랬던, 모델 불안정성이 바로 그것이다.  
대부분의 불안정성은 box의 $(x, y)$ 위치를 predict할 때 발생한다.  
RPN의 경우 값 $t_x$와 $t_y$를 predict하고 $(x, y)$의 중심 좌표를 아래와 같이 계산한다.
$$
\begin{align}
x & = (t_x \times w_a) - x_a \\
y & = (t_y \times h_a) - y_a
\end{align}
$$
예를 들어 $t_x = 1$의 prediction은 anchor box의 width 기준으로 오른쪽으로 옮기는 것이고,  
$t_x = -1$은 같은 기준으로 왼쪽으로 옮기는 것이다.  

이 식은 범위가 제한되지 않아서 예측된 box의 위치와는 관계없이 이미지의 어떤 지점에서든 anchor box가 놓일 수 있다.
이런 모델의 random한 초기화는 합리적인 offset을 안정적으로 predict하기까지 오랜 시간이 걸린다.

offset을 예측하는 것 대신에 YOLO의 접근을 따랐고, grid cell의 위치에 상대적인 위치 좌표를 예측하기로 했다.  
이는 ground truth가 0에서 1 사이에 떨어지도록 제한했다.  
logistic activation을 사용하여, network의 prediction이 이 범위 안에 떨어지도록 제한했다.

network는 output feature map에서 각 grid cell 당 5개의 bounding box를 예측한다.  
network는 각 bounding box의 5개의 좌표, $t_x, t_y, t_w, t_h$ 그리고 $t_o$를 예측한다.  
만약 grid cell이 객체에서 중앙에 있다면, 해당 cell의 좌상단 좌표값 $(c_x, c_y)$이 되고,
bounding box의 width와 height는 $p_w, p_h$가 된다.
$$
\begin{align}
b_x &= \sigma(t_x) + c_x \\
b_y &= \sigma(t_y) + c_y \\
b_w &= p_{w}e^{t_w} \\
b_h &= p_{h}e^{t_h} \\
Pr(object) \times IOU(b, object) &= \sigma(t_o)
\end{align}
$$
조금 더 직관적으로 보기 위해, 아래의 그림을 참고해보자.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_fig3.JPG?raw=true)

location prediction의 범위을 제한했기 때문에, 매개변수화 과정이 더 학습하기 쉬워졌으며 모델을 더 안정적으로 만들었다.  
bounding box의 중심 위치를 직접적으로 예측하는 것과 함께 dimension cluster를 사용하는 것은 anchor box를 사용한 버전에 비해 거의 5% 이상의 성능향상을 보였다.

### Fine-Grained Features

이것은 YOLO가 13 x 13 feature map에서 detecion을 하도록 조정합니다.  
13 x 13 feature map이 큰 객체에서는 충분하지만, 작은 객체들은 더 fine-grained한 featured에서 localize하는 경우에 성능에 이점이 있을 수 있습니다.  
Faster R-CNN과 SSD 모두 network 안의 다양한 feature map에서 proposal network를 통해 여러 해상도에서 detection을 합니다.

YOLO의 경우에는 조금 다른 접근방법을 채택했습니다.  
단순히 passthrough layer (skip connection)을 추가해 26 x 26 크기의 이전 layer를 가져와 처리합니다.  

passthrough layer는 저 해상도 feature와 고 해상도 feature를 공간적 위치 대신 인접한 feature끼리 다른 채널에 concatenate합니다.  
두 feature map의 크기가 다르기 때문에, $(26 \times 26 \times 512)$ 크기의 feature map을 $(13 \times 13 \times 2048)$ 크기의 feature map으로 변환하여 concatenate할 수 있도록 만들어 줍니다.  
detector가 이런 고해상도의 feature에 접근할 수 있기 때문에, 이런 확장된 feature map은 거의 1%의 성능 향상을 가져왔습니다.

### Multi-Scale Training

이전 YOLO는 448 x 448 크기의 입력을 사용했습니다.  
anchor box를 사용하면서 입력 이미지의 크기를 416 x 416으로 바꾸었습니다.
convolutional layer와 pooling layer만을 사용하기 때문에 model이 그때그때마다 입력 이미지를 리사이즈 할 수 있습니다.

저자들은 YOLO v2가 다양한 크기의 이미지에 대해서 강건(robust)하게 작동하기를 바랬기 때문에 이 점을 고려해 모델을 학습했습니다.  

입력 이미지 크기를 고정하는 대신, 10 batch를 학습할 때마다 network가 320부터 608 사이 32배수
값의 입력 이미지 크기를 랜덤하게 선택하여 학습하도록 했습니다.  

이러한 학습 방식이 다양한 입력 차원들에 잘 예측하도록 학습하게 만들었고,  
같은 네트워크임에도 다른 해상도에서 잘 탐지할 수 있음을 의미합니다.  

입력 차원이 줄어듦면 모델 파라미터 수가 줄어들기 때문에,  
다양한 입력 차원에 학습한 YOLO v2 모델은 속도와 정확도 사이의 tradeoff를 조절하기 쉽게 만들어 줍니다.

288 x 288 크기에 작동되는 YOLO v2 모델 같은 경우에는 적은 비용으로 나름 정확한 detector를 만들 수 있으며, 90 FPS에서 Fast R-CNN만큼 어느정도 좋은 mAP 성능을 보인다고 합니다.

반대로 고해상도 YOLO v2 모델의 경우, SOTA detector로 78.6 mAP를 보이면서도 real-time 속도로 작동된다고 합니다.

이에 대한 비교가 Table 3와 Figure 4에 나와있습니다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_table3.JPG?raw=true)

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_fig4.JPG?raw=true)

## Faster

저자들은 detection이 정확할 뿐만 아니라 빠른 속도도 원했습니다.  
대부분의 detection system을 응용하는 것들은 낮은 latency가 필요한 분야들이기 때문에,  
성능을 최대화하기 위해 YOLO v2의 backborn부터 빠른 속도로 작동하도록 디자인했습니다.

YOLO v2의 backborn은 GoogleNet 구조를 기반으로 한 커스텀 구조로,  
VGG-16보다 정확도에서는 ImageNet 224 x 224 기준으로 2% 정도 떨어지지만,  
속도는 더 빠른 **DarkNet-19**를 사용했습니다.

VGG-16과 비슷하게 대부분 3x3 필터를 사용하고, pooling 이후에 채널의 수를 2배로 늘렸습니다.  
또 NIN(Network In Network) 논문에서 사용했던 Global Average pooling을 예측하기 전에 사용하고, 3x3 Convolution 사이마다 1x1 Convolution layer를 사용하여 feature map 채널의 수를 줄였습니다.

Darknet-19의 구조는 아래와 같습니다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_table6.JPG?raw=true)

마지막에 있는 1x1/1000 convolution layer 이후부터는 ImageNet의 1000개 class에 맞춰진 detection을 위한 block이다.

위의 도표는 classification을 위한 network architecture이고,  
실제 detection을 위한 architecture는 도표에서 마지막 convolutional layer를 없애고, 4개의 3x3 / 1024의 convolutional layer를 추가해준다.  
그리고 이어서 1x1 / (5 x 5 x C : classes) 의 convolutional layer를 추가해준다.
그 이유는 각 cell마다 5개의 bounding box가 5개의 좌표 값을 Class마다 예측하기 때문이다.  
그리고 fine-grained feature를 추가하기 위해서, 마지막 3x3 / 512 convolutional layer의 output을 변환하여, 마지막 두번째 3x3 / 1024 convolution layer와 concatenate해준다.

위의 설명을 아래의 그림이 잘 설명해주고 있다.

![https://yeomko.tistory.com/47](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_detection_architecture.png?raw=true)

## Stronger

앞서 설명했듯, YOLO v2는 detection data와 classfication data를 joint training한다.

detection data의 경우에는 bounding box 좌표 예측과 objectness, 공통 객체를 분류하는 것과 같은 detection-specific한 정보를 학습하게 되고,  
classsification data의 경우에는 detect할 수 있는 class 범주의 수를 확장시킨다.

학습 중에 network가 어떤 data를 보고 학습하는지에 따라 역전파되는 부분을 나누었다고 한다.
detection data를 학습하는 경우 전체 YOLO v2 architecture에 역전파를 시켰고,  
classification data를 학습하는 경우에는 classification을 하는 부분에만 역전파를 했다고 한다.

이러한 접근은 문제점에 맞닿뜨리게 되는데,  
detection dataset의 경우에는 '개'와 '보트'와 같은 공통 객체와 일반적인 레이블로 되어 있다.  
반대로 classification dataset의 경우 더 깊고 더 넓은 레이블로 구성되어 있다.  
예를 들어, 단순히 '개'가 아닌 '요크셔 테리어', '베드링턴 테리어'와 같은 견종까지로 말이다.

대부분의 classification에 대한 접근들은 가능한 모든 범주에 대해 softmax를 사용하여 최종 확률 분포를 계산한다.  
softmax를 사용하는 것은 각 class들이 상호 독립적이라는 것을 의미하는데,  
여기서 dataset을 같이 학습하는 것에 대한 문제점이 발생한다.  
예를 들어 ImageNet과 COCO를 함께 학습하고 싶은데, '요크셔 테리어'와 '개'는 상호 독립적이지 않기 때문에 softmax를 사용하는 모델에는 사용할 수 없다.

대신에 multi-lable 모델을 사용하여 상호독립을 가정하지 않는 dataset으로 결합할 수 있다.
이런 접근은 알고 있는 모든 data의 구조들을 위반하게 되는 것이다.  
예를 들어 모든 COCO class들은 상호 독립적이기 때문이다.

### Hierarchical classification

ImageNet 레이블은 WordNet으로부터 도출되었다.  
WordNet은 언어 database로 개념들이 어떻게 연관되어 있는지에 대한 구조를 보여준다.

대부분의 classification에 대한 접근들은 레이블에 대해 수평적인 구조를 가정한다.  
그러나 dataset을 섞기 위해서, WordNet과 같은 구조가 필요하다.

WordNet은 트리가 아니라 연결 그래프로 구성되어 있다.  
WordNet처럼 그래프를 사용하는 것이 아니라 위계적인 트리를 만들어 문제를 단순화했다.

트리를 만들기 위해서, 저자들은 ImageNet 안의 시각적 명사들을 WordNet 그래프에서 찾아 그것의 루트 노드를 찾는다. 이 경우에는 'physical object'가 나온다. (첫 번째로 찾는 루트 노드이기 때문)   
많은 동의어 세트가 그래프에서 단 하나의 경로를 가지기 때문에, 먼저 이러한 경우 모두 트리에 추가해준다.  
그 다음 남기거나 가능한 조금씩 트리를 키워나갈 경로를 더할 개념들을 반복적으로 찾는다.  
만약 개념이 루트 노드로 가는 두 개의 경로(상위 개념이 두 개로 교집합인 case)를 가진다면, 두 경로 중에서 짧은 간선을 만드는 경로만 선택하여 트리에 추가한다.

이런 방식으로 만들어진 것이 WordTree이다.  
시각적 개념들에 대한 위계적인 모델로, classification을 수행할 때 사용된다.
루트 노드부터 그 자식 노드의 조건부 확률을 계속 구해 나가, 확률이 제일 높은 리프 노드를 구하도록 만든다. (그와 동시에 threshold를 넘기는 노드들만)
$$
\begin{align}
&Pr(Norfolk\ terrier|terrier) \\
&Pr(Yorkshire\ terrier|terrier) \\
&Pr(Bedlington\ terrier|terrier) \\
&...
\end{align}
$$

$$
\begin{align}
Pr(Norfolk\ terrier) &= Pr(Norfolk\ terrier |terrier) \\
&* Pr(terrier|hunting\ dog) \\
&* ...* \\
&* Pr(mammal|animal) \\
&* Pr(animal|physical\ object)
\end{align}
$$
즉, 루트 노드부터 출발하여, 자식 노드들에 대한 softmax를 수행하기 때문에,  
각 계층 별로 softmax를 수행한다. 이에 대해서는 Figure 5를 보면 이해하기 쉽다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_fig5.JPG?raw=true)

이러한 방식은 새롭거나 잘 모르겠는 객체 범주에 대해서 성능이 조금만 떨어진다는 것이다.  
예를 들어, '개'에 대한 이미지를 볼 때, 해당 개의 견종이 무엇인지는 몰라도 network는 개라고 예측할 수 있기 때문이다.  (개의 하위어들이 낮은 confidence를 보여 threshold를 넘기지 못하기 때문에)

### Dataset combination with WordTree

WordTree를 사용하면 여러 dataset을 합리적인 방법으로 함께 사용할 수 있다.
트리를 만들 때 사용되는 WordNet은 굉장히 다양해서 이러한 테크닉을 대부분의 dataset에 사용할 수 있으며, Figure 6가 ImageNet과 COCO의 레이블을 혼합하여 WordTree를 만드는 것을 보여준다.

![https://arxiv.org/pdf/1612.08242.pdf](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/yolov2_fig6.JPG?raw=true)

### Joint classification and detection

WordTree를 사용하여 classification과 detecion의 결합 모델을 학습시킬 수 있다.   
저자들은 ImageNet의 전체 배포판 9000개의 class와 COCO detection dataset을 사용하였다.  
위 dataset에 대응하는 WordTree의 class 수는 9418개였다.

ImageNet이 COCO dataset보다 크기가 훨씬 더 컸기 때문에, COCO dataset을 오버샘플링을 하여 두 데이터 비율을 4:1로 맞췄다.

이러한 dataset을 가지고 YOLO9000을 학습시켰다.  
앞서 말한 YOLOv2 기반으로 만들어졌지만, 출력 크기를 제한하기 위해 prior를 5가 아닌 3으로 두었다고 한다.

detection 이미지를 보는 경우에는 오류 역전파를 일반적으로 진행한다.  
classification loss의 경우에는 레이블에 대응하는 수준이나 바로 위의 수준까지만 오류 역전파를 진행했다고 한다. ***(왜냐하면, tree를 타고 내려가면서 예측에 대한 error를 할당하는데, 학습한 class가 개인 경우, 개까지의 loss만 알뿐 그 하위어들에 대한 오류에 대해서는 모르기 때문)***

classification 이미지를 보는 경우에는 classification loss에 대해서만 역전파를 한다.  
이를 위해서 classification 이미지의 class에 대해 가장 높은 probability를 예측한 bounding box를 찾고, 그 예측한 값에 대한 loss를 계산한다.  
이때 ground truth label이 있다면, 예측한 bounding box와의 IoU 값이 적어도 .3 이상이라고 가정하고 objectness loss를 역전파한다.

ImageNet detection task로 YOLO9000을 평가한 결과,  
COCO dataset과 ImageNet detection task가 44개의 범주만 공유함에도 불구하고,  
즉, classification data가 테스트에서 많이 나왔음에도 불구하고 YOLO9000은 19.7 mAP의 성능을 보였으며, 그중 detection data로 나오지 않았던 156개의 객체 범주에 대해서 16.0 mAP의 성능을 보였다고 한다.

YOLO9000은 새로운 종류의 동물을 탐지하는 데에는 성능이 좋았지만, 의류나 장비와 같은 범주에서의 새로운 객체는 잘 탐지하지 못했다.  
COCO dataset에 동물에 대한 정보가 많아 일반화되어 학습할 수 있었지만,  
의류나 장비와 같은 label이 포함되어 있지 않아서 이를 detection하는데에 어려움을 겪는다.

## 내 생각

Object Detection에 대해 완전 초기 논문부터 리뷰하지 않고 YOLOv1부터 시작했지만,  
Object Detection에 대해 파편적으로 공부를 한 것 같아서 부족함을 많이 느꼈던 것 같다.  

Object Detection을 하는 프로젝트를 했던 경험에서 정말 주먹구구식으로 했었는데,  
이번 논문을 리뷰하면서 조금이나마 내가 썼던 SSD 모델에 대해서도 약간의 이해를 할 수 있던 유의미한 공부가 아니였나 싶다.  
프로젝트를 하면서 Mobile 환경에서 작동되어야 하기 때문에 tradeoff에 대해 항상 생각해야만 했고, real-time으로 작동하면서도 유의미하게 인식되어 사용할 수 있는 모델을 만드는 것이 참 어렵겠구나 싶었다.

그런데 이번 논문을 보면서, bottleneck 구조를 가져와 모델 파라미터를 줄이는 점도 흥미로웠지만 제일 흥미로웠던 점은 WordNet에서 WordTree를 만들고, 이를 이용해 classification dataset도 detection model을 학습시키는 데에 사용했다는 점이다.

다음 YOLOv3에서는 어떤 발전이 있을지 궁금해짐과 동시에, 잘 모르는 초기 모델들에 대해서도 공부를 해야겠다는 생각이 들었다.

