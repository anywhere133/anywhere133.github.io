---
title: "YOLO v3  Pytorch로 바닥부터 구현해보기 part.3"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
  - YOLO
use_math: true

---

저번 파트에서 YOLO 구조에서 사용되는 layer들을 구현해보았다.
그리고 이번 파트에서는 PyTorch에서 YOLO 신경망 구조를 구현할 것이다.
구현하고 난 다음에는 입력 이미지에 대해 출력을 만들어 낼 수 있을 것이다.
이번 파트의 목적은 신경망의 순전파를 디자인하는 것이다.

이 튜토리얼은 5개의 파트로 나누어져 있습니다.

Part 1 : [YOLO가 어떻게 작동하는지 이해하기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part1)
Part 2 : [YOLO의 신경망 구조의 layer들을 만들기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part.2/)
Part 3 : 신경망의 순전파 과정을 구현하기.
Part 4 : Objectness Score Thresholding and Non-Maximum Suppression
Part 5 : 입/출력 파이프라인을 설계하기

### 신경망 정의하기

앞서 언급했던 것처럼 PyTorch에서 커스텀 신경망 구조를 만들기 위해 `nn.Module` 클래스를 사용한다.
탐지기를 위한 신경망을 정의해보자.
`darknet.py` 파일 안에서 다음과 같은 클래스를 추가해보자.

```python
class Darknet(nn.Module):
    def __init__(self, cfgfile):
        super(Darknet, self).__init__()
        self.blocks = parse_cfg(cfgfile)
        self.net_info, self.module_list = create_modules(self.blocks)
```

위와 같이 `nn.Module` 클래스를 상속받는 `Darknet`이라는 이름의 클래스를 만들었다.
`Darknet`이라는 신경망 클래스를 `blocks`, `net_info`, `module_list`라는 인스턴스로 초기화했다.

#### 신경망의 순전파 구현하기

신경망의 순전파는 `nn.Module` 클래스의 `forward` 메소드를 override하여 구현할 수 있다.

`forward`는 두 가지 용도로 사용된다.
첫 번째, 출력을 계산하기 위해서.
두 번째, 작업하기 더 쉬운 방식으로 detection feature map의 출력을 변형하기 위해.
(다른 스케일에 걸친 detection map들을 concatenate 할 수 있도록 하기 위해서 변형한다.
그렇게 하지 않는다면 detection map들은 다른 차원을 가지기 때문에 연산이 불가능하다.)

```python
def forward(self, x, CUDA):
    modules = self.blocks[1:]
    outputs = {}   # route layer를 위한 출력 cache
```

`forward`는 3 개의 인자를 받는다.
`self`, 입력 `x` 그리고 GPU 가속을 사용하도록 하는 flag인 `cuda`.

`self.blocks`의 첫 요소가 순전파에서 필요하지 않은 `net` block이기 때문에, 그것을 제외한 `self.blocks[1:]`을 통해 반복문을 돌린다.

*route*와 *shortcut* layer가 이전 layer들로부터의 출력을 필요로 하기 때문에, 매 layer의 출력 feature map을 dict `outputs`에 cache 해놓는다.
`outputs`의 키는 layer들의 인덱스이고, 값은 feature map이다.

`create_modules` 함수가 실행되면, 네트워크의 module들을 포함하고 있는 `model_list`를 반복문을 돌릴 수 있다.
여기서 주목해야 할 점은 module들이 configuration file에서 나타나는 순서대로 list에 추가된다는 것이다.
이 것은 출력을 얻기 위해서 입력을 단순히 각 module을 따라 실행시킬 수 있다는 것이다.

```python
write = 0     # 이 것은 나중에 설명할 것임
for i, module in enumerate(modules):        
    module_type = (module["type"])
```

#### Convolutional and Upsampling Layers

만약 module이 convolutional 혹은 upsample module이라면, 
아래의 코드를 통해 순전파가 작동된다.

```python
        if module_type == "convolutional" or module_type == "upsample":
            x = self.module_list[i](x)
```

#### Route Layer / Shortcut Layer

*route layer*에 대한 코드를 보면, part 2에서 설명했던 두 가지 경우에 대한 처리가 있다.
두 개의 feature map을 concatenate하는 경우에서 두 번째 인자를 1로 받는`torch.cat` 함수를 사용한다.
이것은 feature map을 깊이 방향으로 concatenate하기 원하기 때문이다.
(PyTorch에서는 convolutional layer의 입력과 출력은 (Batch x Channel x Height x Width)의 포맷을 가진다. 깊이와 동일한 Channel 차원을 선택하는 것임)

```python
 elif module_type == "route":
            layers = module["layers"]
            layers = [int(a) for a in layers]

            if (layers[0]) > 0:
                layers[0] = layers[0] - i

            if len(layers) == 1:
                x = outputs[i + (layers[0])]

            else:
                if (layers[1]) > 0:
                    layers[1] = layers[1] - i

                map1 = outputs[i + layers[0]]
                map2 = outputs[i + layers[1]]

                x = torch.cat((map1, map2), 1)

        elif  module_type == "shortcut":
            from_ = int(module["from"])
            x = outputs[i-1] + outputs[i+from_]
```

#### YOLO (Detection Layer)

YOLO의 출력은 feature map의 깊이 방향으로 Bounding Box 속성들을 포함하는 convolutional feature map이다.
cell에 의해 예측되는 bounding box 속성은 각 cell에 따라 하나 씩 쌓여진다.
그래서 만약 (5, 6) 위치의 cell, 두 번째 bounding box에 접근한다면, 
`map[5, 6 (5 + C) : 2 * (5 + C)]`을 통해 그것을 인덱싱할 수 있다.
이 형식은 confidence score에 따라 thresholding하거나, 중앙에 격자 offset을 더하거나 anchor를 적용하는 등의 출력 처리에 있어서 굉장히 불편하다.

다른 문제는 탐지가 3 개의 scale에서 일어나기 때문에, prediction map의 차원은 다를 것이다. 
3 개의 feature map의 차원을 다를지라도, 세 feature map에 처리되는 출력 처리 연산은 유사해야 한다.
3 개의 독립적인 tensor에 각각 연산을 하는 것보다 하나의 tensor에 이러한 연산을 하는 것이 더 좋을 것이다.

이러한 문제들을 해결하기 위해서, `predict_transform` 함수를 도입하자!

### 출력을 변형하기

`predict_transform` 함수는 `util.py` 파일에 존재한다.
그리고 `Darknet` class의 `forward` method를 사용할 때를 위해 함수를 import 해준다.

`util.py`의 상단에 아래의 라이브러리들을 import 해준다.

```python
from __future__ import division

import torch 
import torch.nn as nn
import torch.nn.functional as F 
from torch.autograd import Variable
import numpy as np
import cv2 
```

`predict_transform`은 5 개의 파라미터를 받는다.
`prediction` (신경망의 출력), `inp_dim` (입력 이미지의 차원), `anchors`, `num_classes`, 그리고 선택적인 `CUDA` flag가 있다.

```python
def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):
```

`predict_transform` 함수는 detection feature map을 입력 받아 2-D tensor로 바꾼다.
tensor의 각 줄은 아래의 그림과 같이 bounding box 속성과 동일하다.

![https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-3/](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/bbox_-2.png?raw=true)

즉, 원래의 detection feature map에서 각 cell에 존재하는 bounding box를 행으로 붙인 2d-tensor라고 생각하면 된다.

아래는 위의 변형을 실행하는 코드이다

```python
    batch_size = prediction.size(0)
    stride =  inp_dim // prediction.size(2)
    grid_size = inp_dim // stride
    bbox_attrs = 5 + num_classes
    num_anchors = len(anchors)

    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)
    prediction = prediction.transpose(1,2).contiguous()
    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)
```

anchor의 차원은 `net` block의 `height`와 `width` 속성에 따른다.
이 속성들은 detection map보다 (*stride* 배 만큼) 더 큰 입력 이미지의 차원을 설명한다.
따라서 detection feature map의 stride만큼으로 anchor를 나누어야 한다.

```python
anchors = [(a[0]/stride, a[1]/stride) for a in anchors]
```

part 1에서 이야기했던 방정식에 따르게 출력을 변환해야 한다.

X, Y 좌표와 objectness score를 sigmoid 함수에 넣는다.

```python
    #Sigmoid the  centre_X, centre_Y. and object confidencce
    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])
    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])
    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])
```

중심 좌표 예측에 격자 offsets을 추가한다.

```python
    # Add the center offsets
    grid = np.arange(grid_size)
    # grid로 격자를 만듦. a는 행 기준 나열 / b는 열 기준 나열
    a,b = np.meshgrid(grid, grid)

    # (?, 1) 크기의 tensor로 변환
    x_offset = torch.FloatTensor(a).view(-1,1)
    y_offset = torch.FloatTensor(b).view(-1,1)

    if CUDA:
        x_offset = x_offset.cuda()
        y_offset = y_offset.cuda()

	# 깊이 차원으로 x / y 차원을 concat (grid_size * grid_size, 2), 
	# 그 뒤 dim=1에서 anchor의 개수만큼 반복 (grid_size * grid_size, 2 * num_anchor)
	# 다시 (?, 2) 크기로 변환 (grid_size * grid_size * num_anchor, 2)
    # dim=0에 차원 추가 (1, grid_size * grid_size * num_anchor, 2)
    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)

    prediction[:,:,:2] += x_y_offset
```

bounding box의 차원에 anchor를 적용한다.

```python
    #log space transform height and the width
    anchors = torch.FloatTensor(anchors)

    if CUDA:
        anchors = anchors.cuda()

    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)
    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors
```

class score에 시그모이드 활성화 함수를 적용한다.

```python
    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))
```

이 부분에서 우리가 원하는 마지막 처리는 detection map의 크기를 입력 이미지의 사이즈로 리사이즈하는 것이다.
여기서의 bounding box 속성은 feature map의 크기 (13x13)에 따라 크기가 맞춰져 있다.
만약 입력 사이즈가 416 x 416이면, 속성에 32를 곱하거나 stride 변수를 곱하면 된다

```python
prediction[:,:,:4] *= stride
```

위의 코드를 루프 body에 포함시킨다.

함수의 마지막에서 predictions를 반환한다.

```python
    return prediction
```

#### Detection Layer Revisited

위에서 출력 tensor를 변형했고, 3개의 다른 scale의 detection map을 하나의 큰 tensor로 concatenate할 수 있다. 
이 변형 이전에는 다른 공간적 차원을 가진 feature map을 concatenate하는 것은 불가능했다. 
그러나 지금은 출력 텐서가 bounding box를 행으로 가지는 table처럼 역할을 하기 때문에, concatenation이 가능하다.

이 방법에서의 방해물은 빈 tensor를 만들 수 없고 그 빈 tensor와 (다른 모양의) 비어 있지 않은 tensor와 concatenate할 수 없다는 것이다.
그래서 첫 detection map을 얻기 전 까지 collector(detection을 가지고 있는 tensor)를 만드는 것에 지연이 발생한다. 그러고 나서 이어지는 detection을 얻을 때 map에 concatenate하게 된다.
(즉 detection map이 생긴 다음, detection을 map에 concatenate해야만 하기 때문에,
collector를 먼저 얻을 수는 없다는 것.)

`forward` 함수 안의 loop 바로 앞에 `write = 0`을 볼 수 있을 것이다.
`write` flag는 첫 detection을 했는지 안 했는지를 지시하는 것으로 사용된다.
만약 `write`가 0이면, collector가 만들어 지지 않았다는 것을 의미한다.
반대로 만약 1이라면, collector가 만들어졌다는 것을 의미하고, collector에 detection map을 concatenate 할 수 있다는 것이다.

`predict_transform` 함수를 완성했기 때문에 `forward` 함수 안에 detection feature map을 다루는 코드를 작성할 것이다.

`darknet.py` 파일의 최상단에 아래의 import를 추가해주자.

```python
from util import * 
```

그 다음, `forward` 함수 안에 아래의 코드를 추가한다.

```python
        elif module_type == 'yolo':        

            anchors = self.module_list[i][0].anchors
            #Get the input dimensions
            inp_dim = int (self.net_info["height"])

            #Get the number of classes
            num_classes = int (module["classes"])

            #Transform 
            x = x.data
            x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)
            if not write:              #if no collector has been intialised. 
                detections = x
                write = 1

            else:       
                detections = torch.cat((detections, x), 1)

        outputs[i] = x
```

그리고, 탐색 결과를 반환한다.

```python
    return detections
```

### 순전파 테스트하기.

더미 입력을 만드는 함수가 여기 있다.
지금까지 만든 신경망에 [이 입력 이미지](https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png)을 통과시켜 볼 것이다.
이 함수를 작성하기 전에, 현재 작업하는 경로에 이 이미지를 저장할 것이다.

`darknet.py` 제일 위에 아래와 같은 함수를 정의해보자.

```python
def get_test_input():
    img = cv2.imread("dog-cycle-car.png")
    img = cv2.resize(img, (416,416))          # 입력 차원 리사이즈
    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W 
    img_ = img_[np.newaxis,:,:,:]/255.0       # 0번 dim에 차원 추가 (배치를 위한) | 값 정규화
    img_ = torch.from_numpy(img_).float()     # 배열 안 값의 타입을 float으로 변경
    img_ = Variable(img_)                     # pytorch Variable로 변경
    return img_
```

그 다음, 아래와 같은 코드를 작성한다.

```python
model = Darknet("cfg/yolov3.cfg")
inp = get_test_input()
pred = model(inp, torch.cuda.is_available())
print (pred)
```

위의 코드의 마지막 `print(pred)`로 인해 출력되는 결과를 보면 다음과 같이 출력이 된다.

```python
(  0  ,.,.) = 
   16.0962   17.0541   91.5104  ...     0.4336    0.4692    0.5279
   15.1363   15.2568  166.0840  ...     0.5561    0.5414    0.5318
   14.4763   18.5405  409.4371  ...     0.5908    0.5353    0.4979
               ⋱                ...             
  411.2625  412.0660    9.0127  ...     0.5054    0.4662    0.5043
  412.1762  412.4936   16.0449  ...     0.4815    0.4979    0.4582
  412.1629  411.4338   34.9027  ...     0.4306    0.5462    0.4138
[torch.FloatTensor of size 1x10647x85]
```

이 tensor의 shape은 `1 x 10647 x 85`이다.
첫 번째 차원은 배치 사이즈로, 단순히 이미지를 한 개를 사용했기 때문에 1이라는 값이 나온다.
그리고 배치에서 각 이미지에 대해서 `10647  x 85` 크기의 테이블이 있다.
이 테이블의 각 행은 bounding box를 나타낸다. (4 개의 bounding box 속성들과 1 개의 objectness score, 그리고 80 개 class에 대한 score)

이 점에서 신경망은 랜덤한 가중치들을 가지고, 따라서 올바른 결과를 만들어 내지 못할 것이다.
만든 신경망에 pre-trained 가중치 파일을 적재할 필요가 있다.
이 것을 위해 공식적인 가중치 파일을 사용하여 해결할 것이다.

### Pre-trained 가중치 파일을 다운로드 받기.

detector 경로 안에 가중치 파일을 다운로드 받자.
[여기](https://pjreddie.com/media/files/yolov3.weights)에서 가중치 파일을 받을 수 있다.

### 가중치 파일에 대해 이해하기.

공식 가중치 파일은 serial 형태로 가중치를 저장하는 binary file이다.

가중치를 읽는 것에 있어서 큰 주의가 필요하다.
가중치는 단순히 float으로 저장되어 있고, 그 가중치가 어느 layer에 속하는 것인지 알려주는 것은 전혀 없기 때문이다.
만약 이 과정에서 오류가 생기면, batch norm layer의 가중치가 convolutional layer의 가중치 자리에 적재되는 것과 같은 오류를 막을 수가 없다.
단지 float 값을 읽기 때문에, 어떤 layer에 가중치가 속하는지를 구분할 방법이 없다.
따라서 가중치가 어떻게 저장되어 있는지에 대해 이해해야만 한다.

먼저 가중치는 두 개의 layer 종류에만 속한다.
batch norm layer 또는 convolutional layer 둘 중 하나이다.

두 layer의 가중치는 configuration file 안에서 나타나는 layer의 순서와 정확히 같은 순서로 저장이 되어 있다.
만약 `convolutional(Conv_1)` 블록 뒤에 `shortcut(shortcut_1)` 블록이 있고, 그 뒤에 `convolutional(Conv_3)` 블록이 있으면, 가중치 파일에서는 이전 `convolutional` 블록의 가중치를 포함하고, 이어서 그 뒤 블록들의 가중치들이 따라올 것이라고 예상할 수 있다.

`convolutional` 블록 안에 batch norm layer가 나타날 때에는 biases가 존재하지 않는다.
반면에, batch norm layer가 존재하지 않는 경우에는 bias 가중치가 파일로부터 읽힌다.

아래의 그림이 가중치 파일에 가중치가 어떻게 저장되어 있는지를 종합하고 있다.

![https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-3/](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/wts-1.png?raw=true)

### 가중치 적재하기

가중치를 적재하는 함수를 작성해보자.
함수는 `Darknet` class의 menber 함수, 즉 method가 될 것이다.
`load_weights` method는 weightfile 이라는 가중치 파일의 경로, 하나의 인자를 받는다.

```python
def load_weights(self, weightfile):
```

가중치 파일의 첫 160 바이트는 파일의 헤더를 구성하는 5 개의 `int32` 값이 저장되어 있다.

```python
    # 가중치 파일 열기
    fp = open(weightfile, "rb")

    # 첫 5개 값은 헤더 정보임
    # 1. Major version number
    # 2. Minor Version Number
    # 3. Subversion number 
    # 4,5. Images seen by the network (during training)
    header = np.fromfile(fp, dtype = np.int32, count = 5)
    self.header = torch.from_numpy(header)
    self.seen = self.header[3]
```

나머지 비트들은 위에서 설명한 순서대로 가중치를 나타낸다.
가중치들은 `float32` 또는 32-bit float으로 저장되어 있다.
나머지 가중치를 `np.ndarray`로 적재해보자.

```python
    weights = np.fromfile(fp, dtype = np.float32)
```

가중치 파일을 반복하여, 신경망의 모듈에 가중치를 적재한다.

```python
    ptr = 0
    for i in range(len(self.module_list)):
        module_type = self.blocks[i + 1]["type"]
```

루프 안에서 `convolutional` 블록에 `batch_normalise`가 존재 여부에 대해 먼저 확인해야 한다.
그 것을 기반으로 가중치를 적재한다.

```python
        # 모듈 종류가 convolutional에 적재하는 가중치라면, 아래의 코드를
        # 아니라면 무시한다.
        if module_type == "convolutional":
            model = self.module_list[i]
            # batch_normalize가 block 안에 존재하는지 안하는지 체크하여
		   # 각 경우에 따라 처리
            try:
                batch_normalize = int(self.blocks[i+1]["batch_normalize"])
            except:
                batch_normalize = 0

            conv = model[0]
```

가중치 배열에서 지금 어디인지 확인하기 위한 `ptr` 변수를 유지한다.
만약 `batch_normalize`가 참이면, 아래와 같이 가중치를 적재하게 된다.

```python
        if (batch_normalize):
            bn = model[1]

            # batch norm layer의 가중치 숫자를 가져옴
            num_bn_biases = bn.bias.numel()

            # 가중치 적재
            bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])
            ptr += num_bn_biases

            bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
            ptr  += num_bn_biases

            bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
            ptr  += num_bn_biases

            bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
            ptr  += num_bn_biases

            # 모델 가중치 차원으로 적재한 가중치를 넣기
            bn_biases = bn_biases.view_as(bn.bias.data)
            bn_weights = bn_weights.view_as(bn.weight.data)
            bn_running_mean = bn_running_mean.view_as(bn.running_mean)
            bn_running_var = bn_running_var.view_as(bn.running_var)

            # 모델에 데이터를 복사하기
            bn.bias.data.copy_(bn_biases)
            bn.weight.data.copy_(bn_weights)
            bn.running_mean.copy_(bn_running_mean)
            bn.running_var.copy_(bn_running_var)
```

만약 batch_norm이 False이면, `convolutional` layer의 biase를 그냥 적재한다.

```python
        else:
            # biases의 숫자
            num_biases = conv.bias.numel()

            # 가중치 적재
            conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])
            ptr = ptr + num_biases

            # 모델 가중치 차원에 따라 적재된 가중치를 reshape한다.
            conv_biases = conv_biases.view_as(conv.bias.data)

            # data를 복사한다.
            conv.bias.data.copy_(conv_biases)
```

마지막에 convolutional layer의 가중치를 적재한다.

```python
# convolutional layer에 대한 가중치를 적재한다.
# 가중치의 수
num_weights = conv.weight.numel()

# 가중치에 대해 위에서 했던 것과 동일하게 한다.
# 가중치 적재 및 reshape, 그리고 데이터 복사
conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])
ptr = ptr + num_weights

conv_weights = conv_weights.view_as(conv.weight.data)
conv.weight.data.copy_(conv_weights)
```

이제 darknet 객체 안의 `load_weights` method를 호출하여, `darknet` 객체에서 가중치를 적재할 수 있다.

```python
model = Darknet("cfg/yolov3.cfg")
model.load_weights("yolov3.weights")
```

여기까지 이 파트의 모든 것이다. 모델을 만들고, 가중치를 적재하고, 드디어 객체를 탐지할 수 있게 되었다.

다음 파트에서는 최종적인 탐지 세트를 만들 때 사용되는 objectness confidence thresholding과 non-maximum suppression에 대해 알아볼 것이다.



