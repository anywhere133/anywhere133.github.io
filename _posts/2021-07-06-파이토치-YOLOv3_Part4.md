---
title: "YOLO v3  Pytorch로 바닥부터 구현해보기 part.4"
layout: single
comments: true
categories:
  - Object Detection
  - Deep Learning
tags:
  - 딥러닝
  - Deeplearning
  - Object Detection
  - Pytorch
  - YOLO
  - Non-Maximum Suppression
  - Confidence Thresholding
use_math: true
---

이 글은 YOLO v3 detector를 바닥부터 구현하는 튜토리얼의 4번째 part이다.  
이전 part에서는 신경망의 순전파를 구현했었다.  
이번 part에서는 object confidence에 이어서 non-maximum suppression을 통해 detection을 thresholding할 것이다.  

이 튜토리얼은 5개의 파트로 나누어져 있다.  

Part 1 : [YOLO가 어떻게 작동하는지 이해하기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part1)  
Part 2 : [YOLO의 신경망 구조의 layer들을 만들기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part2/)  
Part 3 : [신경망의 순전파 과정을 구현하기.](https://anywhere133.github.io/object%20detection/deep%20learning/%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98-YOLOv3_Part3/)  
Part 4 : Objectness Score Thresholding and Non-Maximum Suppression  
Part 5 : 입/출력 파이프라인을 설계하기   

이전 파트에서 주어진 입력 이미지에 대해 여러 개의 객체 탐지를 출력하는 모델을 만들었다.  
정확히 모델의 출력은 `B x 10647 x 85` 크기의 텐서가 나온다.  
`B`는 배치 안의 이미지의 수이고, `10647`은 이미지 당 예측된 bounding box의 수이다.  
마지막으로 `85`는 bounding box의 속성 개수이다.  

그러나 Part 1에서 설명했듯, 이 글의 나머지 부분에서 *True detection*으로 호출할 것을 얻기 위해서,   Objectness score thresholding과 Non-maximum suppression을 통해 모델의 출력을 걸러내야 한다.  
그것을 하기 위해서, `util.py` 파일 안에 `write_results` 함수를 만들 것이다.

```python
def write_results(prediction, confidence, num_classes, nms_conf = 0.4):
```

함수는 입력으로 `prediction`, `confidence` (objectness score threshold), `num_classes` (coco dataset 기준 80개), 그리고 `nms_conf` (NMS IoU threshold)를 받는다.

### Object Confidence Thresholding

모델의 예측 텐서는 `B x 10647`개 bounding box들에 대한 정보를 포함하고 있다.  
역치(threshold) 아래의 objectness score를 가지고 있는 각각의 bounding box에 대한 모든 속성(해당 bounding box을 나타내는 전체 행)의 값을 0으로 설정해준다.  

```python
    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)
    prediction = prediction * conf_mask
```

### Non-maximum Suppression 수행하기

**이 부분을 하기 전에, IoU(Intersection over Union)이 무엇인지, 또 Non-Maxium Suppression이 무엇인지에 대해 이해하고 있어야 한다.**  
**그렇지 않으면, 해당 개념에 대해 먼저 알아보고 오자.**

bounding box 속성들은 bounding box의 중심 좌표(center coordinate) 뿐만 아니라, 높이(height)와 넓이(width)도 설명하고 있다.  
그러나 각 box의 대각 좌표 쌍을 사용하여, 두 box의 IoU를 계산하는 것이 더 쉽다.  
따라서 box의 **(center x, center y, height, width)** 속성을 **(top-left corner x, top-left corner y, right-bottom corner x, right-bottom corner y)**으로 변환한다.

(*아래의 코드를 봤을 때 **(center x, center y, width, height)** 순으로 저장되어 있는 것 같음.*)

```python
    box_corner = prediction.new(prediction.shape)
    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)
    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)
    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) 
    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)
    prediction[:,:,:4] = box_corner[:,:,:4]
```

모든 이미지 안의 *true* detection의 수는 다를 수 있다.  
예를 들어 배치 사이즈가 3이고, 그 안에 이미지 1, 2, 3이 5, 2, 4개의 *true* detection 가지고 있다고 하자.  
그러면 confidence thresholding과 NMS를 한 이미지에 대해 한 번씩 수행해야 된다.  
이 것은 벡터화 연산이 수행될 수 없다는 의미이고, `prediction`의 (배치 안에 이미지 인덱스를 포함하고 있는) 첫 번째 차원을 루프 돌려야만 한다는 것이다.

```python
    batch_size = prediction.size(0)

    write = False

    for ind in range(batch_size):
        image_pred = prediction[ind]          #image Tensor
           #confidence threshholding 
           #NMS
```

이전에 설명했듯, `write` flag는 전체 배치를 걸쳐서 *true* detection을 수집하기 위해 사용할 텐서, `output`을 초기화되지 않았다는 것을 알려주는 데에 사용된다. 

처음 루프에 들어가면, 조금은 정리해야 한다.  
각 bounding box 행은 85개의 속성을 가지고 있고, 그 중 80개는 class score 값이다.  
이 지점에서 최대 값을 가지는 class score에 대해서만 걱정하면 된다.  
따라서 각 행에서 80개의 class score를 제거하고,  대신에 최대 값을 가지는 해당 class의 인덱스를 추가해주자.  

```python
        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)
        max_conf = max_conf.float().unsqueeze(1)
        max_conf_score = max_conf_score.float().unsqueeze(1)
        seq = (image_pred[:,:5], max_conf, max_conf_score)
        image_pred = torch.cat(seq, 1)
```

위에서 object confidence가 역치보다 낮은 bounding box 행의 값을 0으로 바꾼 것을 기억하자.
이제 0인 값들을 제거하자.

```python
        non_zero_ind =  (torch.nonzero(image_pred[:,4]))
        try:
            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)
        except:
            continue
        
        #For PyTorch 0.4 compatibility
        #Since the above code with not raise exception for no detection 
        #as scalars are supported in PyTorch 0.4
        if image_pred_.shape[0] == 0:
            continue 
```

try-except 블록은 detection이 없는 경우를 다루기 위해 존재한다.  
이 경우, 이 이미지에 대해 `continue`를 사용해 나머지 loop body를 건너 뛴다.

이미지 안에 탐지된 class를 얻어보자.

```python
        # 이미지 안에 탐지된 다양한 class를 획득
        img_classes = unique(image_pred_[:,-1]) # -1 인덱스가 class 인덱스
```

같은 class에 대한 *true* detection이 여러 개가 있을 수 있기 때문에, 이미지에 나타나는 class를 얻기 위해 `unique` 함수를 사용한다. 

```python
def unique(tensor):
    tensor_np = tensor.cpu().numpy()
    unique_np = np.unique(tensor_np)
    unique_tensor = torch.from_numpy(unique_np)
    
    tensor_res = tensor.new(unique_tensor.shape)
    tensor_res.copy_(unique_tensor)
    return tensor_res
```

그 다음, class 범위의 NMS를 수행한다.

```python
        for cls in img_classes:
            #perform NMS
```

일단 루프 안에 들어가면, 처음 해야 할 일은 특정 class의 탐지를 추출하는 것이다. (변수 `cls`로 표현)

(*아래의 코드는 원래의  파일에서는 3개의 블록으로 나누어져 있었지만, 이 페이지 공간에 한계가 있어서 나누어 놓지 않았다.*)

```python
# 하나의 특정한 class에 대한 detection을 얻는다.
cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)
class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()
image_pred_class = image_pred_[class_mask_ind].view(-1,7)

# 최대 objectness를 갖는 entry가 되도록 detection을 정렬한다.
# confidence는 제일 위에 존재하게 된다.
conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]
image_pred_class = image_pred_class[conf_sort_index]
idx = image_pred_class.size(0)   #Number of detections
```

이제 NMS를 수행한다.

```python
for i in range(idx):
    # 루프에서 보고 있는 box에 뒤이어 오는 모든 박스들의 IoU 값들은 얻는다.
    try:
        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])
    except ValueError:
        break

    except IndexError:
        break

    # IoU > threshold 인 모든 detection에 대해 0으로 처리한다.
    iou_mask = (ious < nms_conf).float().unsqueeze(1)
    image_pred_class[i+1:] *= iou_mask       

    # 0이 아닌 엔트리를 이용하여 예측된 class를 추출.
    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()
    image_pred_class = image_pred_class[non_zero_ind].view(-1,7)
```

여기서 `bbox_iou` 함수를 사용한다.  
첫 입력은 루프 안의 변수 `i`에 의해 인덱싱되는 bounding box 행이다.

`bbox_iou`의 두 번째 입력은 변수 `i`에 인덱싱된 bounding box 행을 제외한 bounding box의 여러 행으로 이루어진 텐서다.  
`bbox_iou` 함수의 출력은 두 번째 입력에 존재하는 각 bounding box와 첫 번째 입력 bounding box의 IoU 값들로 이루어진 텐서이다.

![https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-4/](https://github.com/anywhere133/anywhere133.github.io/blob/master/_posts/picture/bbox-3.png?raw=true)

만약 역치(threshold)보다 더 큰 IoU를 가지는 같은 class의 두 bounding box가 있으면,  
class confidence score가 낮은 bounding box는 제거된다.  
이미 confidence가 더 높은 bounding box가 먼저 오도록 이미 정렬해 놓았다.  
따라서 뒤에 오는 bounding box와의 IoU값이 역치를 넘으면, 뒤에 오는 bbox를 제거한다.

loop body에서 아래의 라인이 `i`보다 더 큰 인덱스 값을 가지는 모든 bbox와 `i`로 인덱스되는 bbox의 IoU 값을 준다. ***(위의 그림이 잘 설명해주고 있다.)***

```python
ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])
```

매 반복마다, 만약 `i`보다 더 큰 인덱스를 가지는 어떤 bbox가 `nms_thresh` 역치 값보다 더 큰 IoU 값을 가지면, 그 특정 bbox는 제거된다.

```python
# IoU > threshold 인 모든 detection에 대해 0으로 처리한다.
iou_mask = (ious < nms_conf).float().unsqueeze(1)
image_pred_class[i+1:] *= iou_mask       

# 0이 아닌 엔트리를 이용하여 예측된 class를 추출.
non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()
image_pred_class = image_pred_class[non_zero_ind].view(-1,7)
```

또한 보다시피, `ious`를 try-except 블록에서 처리되도록 코드를 짜놨다.  
이는 루프가 `idx` (`image_pred_class` 안의 행 개수) 반복으로 돌아가도록 짜여져 있기 때문이다.  
그러나, 루프를 진행할 수록 bbox의 수는 `image_pred_class`에서 줄어들 수 있다.  
이는 `image_pred_class`에서 하나의 값이 제거된다면, `idx` 반복을 할 수 없다는 의미이다.  
따라서 `idx` 값이 *Out of Bounds*(`IndexError`)를 일으키는지, 슬라이스 `image_pred_class[i+1:]`이 `ValueError`를 일으켜 빈 텐서를 반환하는지 확인해야한다.  
이 점에서, NMS가 더 이상의 bbox를 제거할 수 없다는 것을 확신할 수 있고, 루프에서 탈출(`break`)해야 한다.

#### IoU 계산하기.

아래의 코드는 함수 `bbox_iou`이다.

```python
def bbox_iou(box1, box2):
    """
    Returns the IoU of two bounding boxes 
    
    
    """
    # bbox의 좌표 좌상단 좌표쌍 / 우하단 좌표쌍을 얻음.
    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]
    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]
    
    # 교집합 사각형의 좌표값을 얻는다.
    inter_rect_x1 =  torch.max(b1_x1, b2_x1)
    inter_rect_y1 =  torch.max(b1_y1, b2_y1)
    inter_rect_x2 =  torch.min(b1_x2, b2_x2)
    inter_rect_y2 =  torch.min(b1_y2, b2_y2)
    
    # 교집합 영역을 계산한다.
    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)
 
    # 합집합 영역을 계산한다.
    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)
    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)
    
    # 교집합 / 합집합 - 교집합 을 통해 IoU 값을 계산한다.
    iou = inter_area / (b1_area + b2_area - inter_area)
    
    return iou
```

### 예측(Prediction) 작성하기

함수 `write_results`의 출력은 `D x 8` 크기의 텐서이다.  
여기서 `D`는 모든 이미지에서의 *true* detection의 수이며, 각각은 행으로 표현된다.  
각 detection은 8개의 속성들을 가지며, detection이 속하는 ***배치 안에서의 이미지 인덱스***,  
***4개의 모서리 좌표들(좌상단 / 우하단 좌표쌍)***, ***objectness score 값***, ***최대 confidence를 보인 class의 score***, 그리고 ***해당 class의 인덱스***로 구성되어 있다.

바로 직전에, 출력 텐서에 detection을 할당하기 전까지는 출력 텐서를 초기화하지 않는다고 했다.  
한번 초기화되면, 출력 텐서에 이어지는 detection을 concatenate한다.  
`write` flag를 사용하여, 텐서가 초기화되었는지 아닌지를 확인한다.  
class를 걸쳐 반복하는 루프의 끝에서 최종 detection을 `output` 텐서에 더한다.

```python
            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      
            # Repeat the batch_id for as many detections of the class cls in the image
            seq = batch_ind, image_pred_class

            if not write:
                output = torch.cat(seq,1)
                write = True
            else:
                out = torch.cat(seq,1)
                output = torch.cat((output,out))
```

함수의 마지막에서, `output`이 초기화되었는지 아닌지에 대해 체크한다.  
만약 초기화되지 않았다는 것은 배치의 어떤 이미지에서도 detection이 하나도 존재하지 않는다는 것을 의미한다.  
이 경우에는, 0을 반환한다.

```python
    try:
        return output
    except:
        return 0
```

여기까지가 이 글의 마지막이다.  
이 글의 마지막에서, 행으로 각 예측을 가지고 있는 형태의 tensor로 최종 예측을 가지게 되었다.  
이제는 디스크에서 이미지를 읽고, 예측을 수행하고, 이미지에 bbox를 그리고, 이 이미지를 보여주고 저장하는 입력 파이프라인을 만드는 것만이 남았다.  
이 것이 다음 파트에서 할 것이다!
